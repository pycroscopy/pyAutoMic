{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment Anything Model (SAM) - DIFFRACTION(ceta): live workflow\n",
    "- Acquire an image using HAADF detector\n",
    "- Feed HAADF image in SAM pipeline:\n",
    "        - get particles with segmented masks\n",
    "        - find center point of each particle\n",
    "- Acquire diffraction(ceta) detector signals at center of each particle\n",
    "#### Contributor(s): Utkarsh Pratiush <utkarshp1161@gmail.com> - 31st March 2025\n",
    "#### edited - \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stemOrchestrator.logging_config   import setup_logging\n",
    "data_folder  = \".\"\n",
    "out_path = data_folder\n",
    "setup_logging(out_path=out_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stemOrchestrator.acquisition import TFacquisition, DMacquisition\n",
    "from stemOrchestrator.simulation import DMtwin\n",
    "from stemOrchestrator.process import HAADF_tiff_to_png, tiff_to_png\n",
    "from autoscript_tem_microscope_client import TemMicroscopeClient\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "plot = plt\n",
    "from typing import Dict\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########SAM part ********************************************************************************************************\n",
    "\n",
    "from stemOrchestrator.MLlayer.MLlayerSAM import setup_device, download_sam_model, initialize_sam_model, preprocess_image, generate_and_save_masks, create_normalized_particle_positions, display_image_with_masks, display_image_with_labels, extract_mask_contours, generate_mask_colors, visualize_masks_with_boundaries, extract_particle_data, print_boundary_points_info, plot_centroids, sample_particle_positions, plot_sampled_positions, create_visualization_with_masks\n",
    "import pickle\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray \n",
    "from typing import List, Dict, Union\n",
    "\n",
    "\n",
    "def run_sam(image_data: np.ndarray, path_folder: str) -> Union[List, Dict]:\n",
    "    \"\"\"Main function to run SAM segmentation pipeline.\"\"\"\n",
    "    device = setup_device()\n",
    "    \n",
    "    model_type = \"vit_b\"  # Options: 'vit_b', 'vit_l', 'vit_h'\n",
    "    checkpoint_url = \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\"\n",
    "    checkpoint_path = \"sam_vit_b_01ec64.pth\"\n",
    "    download_sam_model(model_type, checkpoint_url, checkpoint_path)\n",
    "    sam, mask_generator = initialize_sam_model(model_type, checkpoint_path, device)\n",
    "    img_np = preprocess_image(image_data)\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(img_np)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # Generate and visualize masks\n",
    "    masks_path = f'{path_folder}/masks_Au_online.pkl'\n",
    "    masks = generate_and_save_masks(mask_generator, img_np, masks_path)\n",
    "    visual_image, centroids = create_visualization_with_masks(img_np, masks)\n",
    "    display_image_with_masks(visual_image, \"Image with Segmentation Masks\")\n",
    "    display_image_with_labels(visual_image, centroids, \"Image with Segmentation Masks and Labels\")\n",
    "    \n",
    "    mask_contours = extract_mask_contours(masks)\n",
    "    mask_colors = generate_mask_colors(len(masks))\n",
    "    boundaries_path = f\"{path_folder}/Segmentation Masks with Boundaries and Centroids.png\"\n",
    "    visualize_masks_with_boundaries(visual_image, centroids, mask_contours, mask_colors, boundaries_path)\n",
    "    particles = extract_particle_data(masks)\n",
    "    # Save particle data\n",
    "    with open(f'{path_folder}/particles.pkl', 'wb') as f:\n",
    "        pickle.dump(particles, f)\n",
    "    \n",
    "    print_boundary_points_info(particles)\n",
    "    centroids_array = np.array(centroids)\n",
    "    plot_centroids(centroids_array, img_np)\n",
    "    positions_sampled = sample_particle_positions(particles, img_np)\n",
    "    plot_sampled_positions(positions_sampled, img_np, len(centroids))\n",
    "    each_particle_position = create_normalized_particle_positions(particles, img_np.shape[:2])\n",
    "    with open(f'{path_folder}/sampled_boundary_pts_particles.pkl', 'wb') as f: # Save normalized particle positions\n",
    "        pickle.dump(each_particle_position, f)\n",
    "    \n",
    "    all_particle_keys = each_particle_position.keys()\n",
    "\n",
    "\n",
    "    \n",
    "    print(\"Processing complete!\")\n",
    "    return all_particle_keys, each_particle_position\n",
    "\n",
    "##########****************************************************************************************************************************\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config :Dict) -> None:\n",
    "    ip = config[\"ip\"]\n",
    "    port = config[\"port\"]\n",
    "    haadf_exposure = config[\"haadf_exposure\"]\n",
    "    haadf_resolution = config[\"haadf_resolution\"]\n",
    "    out_path = config[\"out_path\"]\n",
    "    setup_logging(out_path=out_path)\n",
    "    \n",
    "\n",
    "    microscope_tf = TemMicroscopeClient()\n",
    "    microscope_tf.connect(ip, port = port)# 7521 on velox  computer\n",
    "    microscope_dm = DMtwin()\n",
    "    # query state:\n",
    "\n",
    "    global tf_acquisition\n",
    "    tf_acquisition = TFacquisition(microscope=microscope_tf, offline=True)\n",
    "    # dm_acquisition = DMacquisition(microscope=microscope_dm, offline=True)\n",
    "\n",
    "    # Get haadf\n",
    "    haadf_np_array, haadf_tiff_name = tf_acquisition.acquire_haadf(exposure = haadf_exposure, resolution=haadf_resolution)\n",
    "\n",
    "    HAADF_tiff_to_png(haadf_tiff_name)\n",
    "    logging.info(\"END acquisition.\")\n",
    "    \n",
    "    all_particle_keys, each_particle_position = run_sam(haadf_np_array, out_path)\n",
    "    \n",
    "    \n",
    "    def acquire_and_plot_combined(image_data, particle_key, ceta_exposure):\n",
    "        \"\"\"Acquire EDS spectrum, CETA image, and plot all three components in a single figure.\"\"\"\n",
    "        \n",
    "        \n",
    "        # lets do just eels at boundary points\n",
    "        print(f\"cbed at centres for particle{particle_key}\")\n",
    "        # Ensure the directory exists\n",
    "        directory = f'{out_path}/particle{particle_key}'\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        # Acquire the CETA image\n",
    "        # ceta_cam.insert()\n",
    "        wait_sec = 2\n",
    "        # print(f\"ceta-wait insert{wait_sec} sec\")\n",
    "        # time.sleep(wait_sec)\n",
    "        ceta_cp_array, ceta_tiff_name = tf_acquisition.acquire_ceta_or_flucam(exposure=ceta_exposure, resolution=1024, camera=\"ceta\")\n",
    "        tiff_to_png(ceta_tiff_name)\n",
    "        \n",
    "        \n",
    "        img = ceta_cp_array - np.min(ceta_cp_array)\n",
    "        ceta_image_data = (255 * (img / np.max(img))).astype(np.uint8)\n",
    "        n = ceta_image_data.shape[0]# 4096\n",
    "        # center_half = ceta_image_data[n // 4: 3 * n // 4, n // 4: 3 * n // 4]\n",
    "        # center_quarter = ceta_image_data[n // 2: 3 * n // 4, n // 2: 3 * n // 4]\n",
    "        center_quarter = ceta_image_data[1024:-1024, 1024:-1024]\n",
    "        \n",
    "        # Get the current beam position\n",
    "        position = tf_acquisition.query_paused_beam_positon()\n",
    "        x = position.x\n",
    "        y = position.y\n",
    "        formatted_position = f\"({x:.2g}, {y:.2g})\"\n",
    "\n",
    "        # Create a figure with three subplots\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "        # Plot the acquired image data\n",
    "        axs[0].imshow(image_data, cmap='gray')\n",
    "        axs[0].set_title('Acquired Image')\n",
    "        axs[0].set_axis_off()  # Hide axes for the image plot\n",
    "        axs[0].scatter(x * image_data.shape[0], y * image_data.shape[1], c='r', s=100, marker='x', label=f\"Position: {formatted_position}\")\n",
    "        axs[1].imshow(np.log(center_quarter + 1), cmap='gray')  # Using log contrast\n",
    "        axs[1].set_title(f'Acquired CETA Image at Position: {formatted_position}')\n",
    "        axs[1].set_axis_off()  # Hide axes for the image plot\n",
    "\n",
    "\n",
    "        # Adjust layout and display\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{out_path}/particle{particle_key}/haadf_cbed_at_centroid of particle{particle_key}_{formatted_position}.png', dpi = 300)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "        # save the ceta and spectrum data\n",
    "        np.save(f'{out_path}/particle{particle_key}/ceta image at position {formatted_position}.npy', ceta_image_data)# ceta image save\n",
    "        # ceta_image.save(f'{out_path}/ceta_raw_ at position {formatted_position}.tiff')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def run_acquisition_for_particle(image_data, particle_key, particle_dict, ceta_exposure):\n",
    "        \"\"\"Run the entire acquisition process for a given position.\"\"\"\n",
    "        position = list(particle_dict[particle_key][\"centroid\"])\n",
    "        tf_acquisition.move_paused_beam(position[0], position[1])\n",
    "        acquire_and_plot_combined(image_data, particle_key, ceta_exposure)\n",
    "\n",
    "\n",
    "    spec_metadata_dict = {\"ceta_exposure_sec\" : 0.1}\n",
    "\n",
    "    ceta_exposure = spec_metadata_dict[\"ceta_exposure_sec\"]\n",
    "\n",
    "    file_path = f'{out_path}/spectrometer_metadata_dict.pkl'\n",
    "    # Save the dictionary to disk\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(spec_metadata_dict, file)\n",
    "    print(f\"spectrometer_metadata_dictsaved to {file_path}\")\n",
    "\n",
    "    for particle_key in all_particle_keys:\n",
    "        run_acquisition_for_particle(haadf_np_array,particle_key, each_particle_position, ceta_exposure)# 25 seconds per reading: --> 1min 35 seconds for 4 points\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the microscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "ip = os.getenv(\"MICROSCOPE_IP\")\n",
    "port = os.getenv(\"MICROSCOPE_PORT\")\n",
    "\n",
    "if not ip or not port:\n",
    "    secret_path = Path(\"../../config_secret.json\")\n",
    "    if secret_path.exists():\n",
    "        with open(secret_path, \"r\") as f:\n",
    "            secret = json.load(f)\n",
    "            ip = ip or secret.get(\"ip_TF_sim\")\n",
    "            port = port or secret.get(\"port_TF_sim\")\n",
    "\n",
    "\n",
    "if not ip:\n",
    "    ip = input(\"Enter microscope IP: \")\n",
    "if not port:\n",
    "    port = input(\"Enter microscope Port: \")\n",
    "\n",
    "try:\n",
    "    port = int(port)\n",
    "except ValueError:\n",
    "    raise ValueError(\"Port must be an integer\")\n",
    "\n",
    "config = {\n",
    "    \"ip\": ip,\n",
    "    \"port\": port,\n",
    "    \"haadf_exposure\": 40e-8,  # micro-seconds per pixel\n",
    "    \"haadf_resolution\": 512, # square\n",
    "    \"out_path\": \".\"\n",
    "}\n",
    "\n",
    "main(config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dmclient_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
