{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07b92849",
   "metadata": {},
   "source": [
    "# Q. What to Do with Multimodal Data (HAADF + EDX)\n",
    "\n",
    "## **Given**\n",
    "\n",
    "* **HAADF image:**\n",
    "  `haadf` → `np.array` of shape `(512, 512)`\n",
    "* **Pixel positions:**\n",
    "  `pixel_pos = [(x₁, y₁), (x₂, y₂), (x₃, y₃), ...]`\n",
    "* **EDX spectra:**\n",
    "  Each position `(xₙ, yₙ)` has an associated **2048-dimensional spectrum** (e.g., counts vs. energy).\n",
    "\n",
    "\n",
    "1. **Build the data matrix:**\n",
    "   Combine all spectra into an array of shape `(n, 2048)`.\n",
    "\n",
    "2. **Dimensionality reduction (PCA):**\n",
    "\n",
    "   * Apply PCA to reduce:\n",
    "\n",
    "     ```\n",
    "     (n, 2048) → (n, k)\n",
    "     ```\n",
    "\n",
    "     where `k ∈ {1, 2, 3}` depending on visualization needs.\n",
    "   * Example:\n",
    "\n",
    "     * `k = 1` → single score per spectrum (scalar intensity)\n",
    "     * `k = 2 or 3` → scatter visualization in PCA space\n",
    "\n",
    "3. **Visualization:**\n",
    "\n",
    "   * **PCA score plots:**\n",
    "     Plot the reduced components `(PC1, PC2, PC3)` to inspect clusters.\n",
    "   * **Spatial overlay:**\n",
    "\n",
    "     * Plot `pixel_pos` over the HAADF image.\n",
    "     * Color each point by its **first PCA score (PC1)** to visualize spatial variation in spectral features.\n",
    "\n",
    "   ```python\n",
    "   plt.imshow(haadf, cmap='gray')\n",
    "   plt.scatter(x, y, c=pca_scores[:, 0], cmap='viridis', s=10)\n",
    "   plt.title(\"HAADF + PCA(EDX) Overlay\")\n",
    "   plt.colorbar(label=\"PC1 Score\")\n",
    "   plt.show()\n",
    "   ```\n",
    "\n",
    "\n",
    "**(Optional Enhancements)**\n",
    "\n",
    "*  Use **clustering** (e.g., KMeans, GMM) on PCA scores to estimate the number of distinct spectrum types.\n",
    "*  Visualize **cluster maps** to correlate spectral regions with physical structures in the HAADF image.\n",
    "*  If needed, normalize spectra before PCA:\n",
    "\n",
    "  ```python\n",
    "  spectrum = (spectrum - np.min(spectrum)) / np.ptp(spectrum)\n",
    "  ```\n",
    "*  Store results as:\n",
    "\n",
    "  * `pca_scores.npy`\n",
    "  * `cluster_labels.npy`\n",
    "  * `overlay_plot.png`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efdf5de",
   "metadata": {},
   "source": [
    "## 1. Dummy workflow\n",
    "- Simulated multimodal dataset (HAADF + EDX) with PCA, clustering, and overlays\n",
    "- Generates a synthetic HAADF image (512x512)\n",
    "- Samples N random pixel positions\n",
    "- Simulates a 2048-dim EDX spectrum at each position for 3 \"phases\"\n",
    "- Runs PCA and KMeans on spectra\n",
    "- Plots: HAADF + PCA(PC1) overlay, PCA scatter with clusters\n",
    "- Saves: pca_scores.npy, cluster_labels.npy, overlay_plot.png, pca_scatter.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd82beb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "rng = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b230248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53941dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1) Simulate a HAADF image\n",
    "# -----------------------------\n",
    "H, W = 512, 512\n",
    "\n",
    "\n",
    "# Create 3 smooth \"domains\" via Gaussian blobs\n",
    "def gaussian_2d(x, y, cx, cy, sx, sy):\n",
    "    return np.exp(-(((x - cx) ** 2) / (2 * sx**2) + ((y - cy) ** 2) / (2 * sy**2)))\n",
    "\n",
    "\n",
    "yy, xx = np.mgrid[0:H, 0:W]\n",
    "blob1 = 0.9 * gaussian_2d(xx, yy, 150, 170, 90, 120)\n",
    "blob2 = 0.7 * gaussian_2d(xx, yy, 360, 320, 80, 80)\n",
    "blob3 = 0.5 * gaussian_2d(xx, yy, 260, 120, 60, 50)\n",
    "\n",
    "haadf = blob1 + blob2 + blob3\n",
    "haadf += 0.08 * rng.normal(size=(H, W))\n",
    "haadf = np.clip(haadf, 0, None)\n",
    "haadf /= haadf.max()\n",
    "\n",
    "plt.imshow(haadf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdef418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 2) Sample pixel positions\n",
    "# -----------------------------\n",
    "N = 30  # keep this modest to stay fast\n",
    "xs = rng.integers(0, W, size=N)\n",
    "ys = rng.integers(0, H, size=N)\n",
    "pixel_pos = np.stack([xs, ys], axis=1)\n",
    "\n",
    "print(\"number of position\", N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1c27be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 3) Simulate 3-phase EDX spectra\n",
    "# -----------------------------\n",
    "num_channels = 2048\n",
    "channels = np.linspace(0, 20.0, num_channels)  # e.g., keV axis (0-20 keV)\n",
    "\n",
    "\n",
    "def make_phase_template(peaks_keV, widths, amplitudes):\n",
    "    \"\"\"Create a spectral template with multiple Gaussian peaks.\"\"\"\n",
    "    spec = np.zeros_like(channels)\n",
    "    for mu, w, a in zip(peaks_keV, widths, amplitudes):\n",
    "        spec += a * np.exp(-0.5 * ((channels - mu) / w) ** 2)\n",
    "    # Add a mild background\n",
    "    spec += 0.02 + 0.02 * (channels / channels.max())\n",
    "    return spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9995e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 3 \"phases\" with different peak patterns\n",
    "template_A = make_phase_template(\n",
    "    peaks_keV=[2.2, 5.0, 8.6], widths=[0.12, 0.18, 0.22], amplitudes=[1.0, 0.7, 0.9]\n",
    ")\n",
    "template_B = make_phase_template(\n",
    "    peaks_keV=[1.0, 6.4, 9.7, 12.0],\n",
    "    widths=[0.15, 0.20, 0.18, 0.25],\n",
    "    amplitudes=[0.8, 1.0, 0.6, 0.7],\n",
    ")\n",
    "template_C = make_phase_template(\n",
    "    peaks_keV=[3.5, 7.2, 15.0], widths=[0.16, 0.22, 0.30], amplitudes=[0.9, 0.8, 0.9]\n",
    ")\n",
    "\n",
    "templates = np.stack([template_A, template_B, template_C], axis=0)\n",
    "templates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebfd6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map position -> phase probability using the HAADF intensity + location\n",
    "# (just a heuristic to produce structured spatial phase regions)\n",
    "norm_h = haadf[ys, xs]\n",
    "pA = 0.6 * norm_h + 0.2 * (xs / W) + 0.2 * (1 - ys / H)\n",
    "pB = 0.5 * (1 - norm_h) + 0.3 * (ys / H) + 0.2 * (xs / W)\n",
    "pC = 1.0 - (pA + pB)\n",
    "\n",
    "# Stabilize & normalize\n",
    "stack = np.stack([pA, pB, pC], axis=1)\n",
    "stack = np.maximum(stack, 1e-4)\n",
    "stack = stack / stack.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fdb162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a hard phase label per pixel\n",
    "phase_labels_true = np.array([rng.choice(3, p=stack[i]) for i in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a25e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate spectra with count scaling and noise\n",
    "def simulate_spectrum(phase_idx, count_scale):\n",
    "    base = templates[phase_idx]\n",
    "    # scale counts (like dwell time / thickness effects)\n",
    "    spec = count_scale * base\n",
    "    # Poisson-like noise (approximate)\n",
    "    noisy = rng.poisson(lam=np.clip(spec * 200.0, 1e-3, None)).astype(\n",
    "        float\n",
    "    )  # scale up, then noise\n",
    "    noisy /= 200.0\n",
    "    # slight baseline drift\n",
    "    drift = 1.0 + 0.05 * rng.normal()\n",
    "    return np.clip(noisy * drift, 0, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068519bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectra = np.empty((N, num_channels), dtype=np.float32)\n",
    "for i in range(N):\n",
    "    phase = phase_labels_true[i]\n",
    "    # vary scale by local HAADF intensity\n",
    "    scale = 0.8 + 0.6 * norm_h[i]\n",
    "    spectra[i] = simulate_spectrum(phase, scale)\n",
    "\n",
    "spectra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4934b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: normalize spectra (safe min-max per spectrum)\n",
    "mins = spectra.min(axis=1, keepdims=True)\n",
    "ptps = np.ptp(spectra, axis=1, keepdims=True)\n",
    "ptps = np.where(ptps == 0, 1.0, ptps)\n",
    "spectra_norm = (spectra - mins) / ptps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb019dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 4) PCA and KMeans\n",
    "# -----------------------------\n",
    "k = 3\n",
    "pca = PCA(n_components=k, random_state=42)\n",
    "scores = pca.fit_transform(spectra_norm)  # shape (N, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa85241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMeans on PCA scores\n",
    "km = KMeans(n_clusters=3, n_init=10, random_state=42)\n",
    "cluster_labels = km.fit_predict(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dabe0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 5) Plots\n",
    "# -----------------------------\n",
    "\n",
    "# a) HAADF overlay with PC1 color\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(haadf)\n",
    "plt.scatter(xs, ys, c=scores[:, 0], s=6)\n",
    "plt.title(\"HAADF + EDX PCA Overlay (PC1 color)\")\n",
    "plt.colorbar(label=\"PC1 score\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"/mnt/data/overlay_plot.png\", dpi=160)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c212b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) PCA scatter with clusters\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(scores[:, 0], scores[:, 1], s=8, alpha=0.8, c=cluster_labels)\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"PCA Scatter of EDX Spectra with KMeans Clusters\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"/mnt/data/pca_scatter.png\", dpi=160)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb683ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 6) Save artifacts\n",
    "# -----------------------------\n",
    "# np.save(\"/mnt/data/pca_scores.npy\", scores)\n",
    "# np.save(\"/mnt/data/cluster_labels.npy\", cluster_labels)\n",
    "# np.save(\"/mnt/data/pixel_pos.npy\", pixel_pos)\n",
    "# np.save(\"/mnt/data/haadf.npy\", haadf)\n",
    "# np.save(\"/mnt/data/spectra_norm.npy\", spectra_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acdcc78",
   "metadata": {},
   "source": [
    "## 2. Dummy workflow 2 with random arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fa1908",
   "metadata": {},
   "outputs": [],
   "source": [
    "H, W = 512, 512\n",
    "\n",
    "# get haadf- a random numpy array of size 512*512\n",
    "haadf = np.random.random(size=(H, W))\n",
    "plt.imshow(haadf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09764d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the HAADF image\n",
    "\n",
    "import pandas as pd\n",
    "from skimage import exposure\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(haadf)\n",
    "plt.title(\"Synthetic HAADF (512x512)\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"/mnt/data/haadf.png\", dpi=160)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# b) Plot pixel distribution\n",
    "# -----------------------------\n",
    "def plot_hist(arr, title):\n",
    "    arr_flat = arr.ravel()\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.hist(arr_flat, bins=100)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Pixel value\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_hist(haadf, \"Histogram: Original HAADF\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# c) Normalizations\n",
    "# -----------------------------\n",
    "def safe_minmax(a):\n",
    "    a = a.astype(float)\n",
    "    mn = np.nanmin(a)\n",
    "    ptp = np.ptp(a)\n",
    "    ptp = ptp if ptp != 0 else 1.0\n",
    "    return (a - mn) / ptp\n",
    "\n",
    "\n",
    "def zscore_rescaled(a):\n",
    "    a = a.astype(float)\n",
    "    mu = np.nanmean(a)\n",
    "    sd = np.nanstd(a)\n",
    "    sd = sd if sd != 0 else 1.0\n",
    "    z = (a - mu) / sd\n",
    "    return safe_minmax(z)\n",
    "\n",
    "\n",
    "def robust_zscore_rescaled(a):\n",
    "    a = a.astype(float)\n",
    "    med = np.nanmedian(a)\n",
    "    mad = np.nanmedian(np.abs(a - med))\n",
    "    scale = 1.4826 * mad if mad != 0 else 1.0\n",
    "    z = (a - med) / scale\n",
    "    return safe_minmax(z)\n",
    "\n",
    "\n",
    "def percentile_minmax(a, lo=1, hi=99):\n",
    "    a = a.astype(float)\n",
    "    p1, p99 = np.nanpercentile(a, [lo, hi])\n",
    "    denom = max(p99 - p1, 1e-9)\n",
    "    return np.clip((a - p1) / denom, 0, 1)\n",
    "\n",
    "\n",
    "def hist_equalize(a):\n",
    "    a = safe_minmax(a)\n",
    "    return exposure.equalize_hist(a)\n",
    "\n",
    "\n",
    "def clahe(a, clip=0.01):\n",
    "    a = safe_minmax(a)\n",
    "    return exposure.equalize_adapthist(a, clip_limit=clip)\n",
    "\n",
    "\n",
    "def background_subtract(a, sigma=20):\n",
    "    a = a.astype(float)\n",
    "    bg = ndi.gaussian_filter(a, sigma=sigma)\n",
    "    res = a - bg\n",
    "    return safe_minmax(res)\n",
    "\n",
    "\n",
    "def log_compress(a, gain=5.0):\n",
    "    a = safe_minmax(a)\n",
    "    return np.log1p(gain * a) / np.log1p(gain)\n",
    "\n",
    "\n",
    "def gamma_correct(a, gamma=0.6):\n",
    "    a = safe_minmax(a)\n",
    "    return np.power(a, gamma)\n",
    "\n",
    "\n",
    "def lcn(a, size=15):\n",
    "    a = a.astype(float)\n",
    "    mu = ndi.uniform_filter(a, size=size)\n",
    "    sd = np.sqrt(ndi.uniform_filter((a - mu) ** 2, size=size)) + 1e-6\n",
    "    z = (a - mu) / sd\n",
    "    return safe_minmax(z)\n",
    "\n",
    "\n",
    "# Simulate a flat-field (gain) pattern and correct\n",
    "def flat_field_correct(a):\n",
    "    a = a.astype(float)\n",
    "    # synthetic spatial sensitivity\n",
    "    yy, xx = np.mgrid[0 : a.shape[0], 0 : a.shape[1]]\n",
    "    sens = 0.8 + 0.2 * (np.sin(xx / 40.0) * np.cos(yy / 50.0))\n",
    "    dark = 0.02  # small offset\n",
    "    corrected = (a - dark) / np.maximum(sens, 1e-6)\n",
    "    return safe_minmax(corrected)\n",
    "\n",
    "\n",
    "normalizations = {\n",
    "    \"minmax\": safe_minmax,\n",
    "    \"zscore_rescaled\": zscore_rescaled,\n",
    "    \"robust_zscore_rescaled\": robust_zscore_rescaled,\n",
    "    \"percentile_minmax_1_99\": lambda a: percentile_minmax(a, 1, 99),\n",
    "    \"hist_equalize\": hist_equalize,\n",
    "    \"clahe\": clahe,\n",
    "    \"background_subtract_sigma20\": background_subtract,\n",
    "    \"log_compress_gain5\": log_compress,\n",
    "    \"gamma_0.6\": gamma_correct,\n",
    "    \"lcn_size15\": lcn,\n",
    "    \"flat_field_correct\": flat_field_correct,\n",
    "}\n",
    "\n",
    "results = {}\n",
    "# out_dir = Path(\"/mnt/data\")\n",
    "# out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Apply each normalization, plot histogram and save normalized image\n",
    "for name, func in normalizations.items():\n",
    "    try:\n",
    "        arr = func(haadf)\n",
    "        arr = np.nan_to_num(arr, nan=0.0, posinf=1.0, neginf=0.0)\n",
    "        arr = np.clip(arr, 0, 1)  # ensure comparable domain\n",
    "        results[name] = arr\n",
    "\n",
    "        # Save image\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.imshow(arr)\n",
    "        plt.title(f\"Normalized: {name}\")\n",
    "        plt.tight_layout()\n",
    "        # plt.savefig(str(out_dir / f\"norm_{name}.png\"), dpi=160)\n",
    "        plt.show()\n",
    "\n",
    "        # Plot histogram\n",
    "        plot_hist(arr, f\"Histogram: {name}\")\n",
    "    except Exception as e:\n",
    "        # record failure\n",
    "        results[name] = None\n",
    "        print(f\"[WARN] {name} failed with error: {e}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# d) Score and choose best-distributed\n",
    "# -----------------------------\n",
    "def histogram_entropy(a, bins=256):\n",
    "    h, edges = np.histogram(a.ravel(), bins=bins, range=(0, 1), density=True)\n",
    "    h = h + 1e-12  # avoid log(0)\n",
    "    p = h / h.sum()\n",
    "    return -np.sum(p * np.log(p))\n",
    "\n",
    "\n",
    "def spread_score(a):\n",
    "    # Combined score: entropy + normalized std\n",
    "    ent = histogram_entropy(a, bins=256)\n",
    "    std = float(np.std(a))\n",
    "    return ent + std, ent, std\n",
    "\n",
    "\n",
    "rows = []\n",
    "for name, arr in results.items():\n",
    "    if arr is None:\n",
    "        rows.append((name, np.nan, np.nan, np.nan))\n",
    "    else:\n",
    "        total, ent, std = spread_score(arr)\n",
    "        rows.append((name, total, ent, std))\n",
    "\n",
    "df = pd.DataFrame(rows, columns=[\"method\", \"score_total(ent+std)\", \"entropy\", \"std\"])\n",
    "df_sorted = df.sort_values(\"score_total(ent+std)\", ascending=False).reset_index(\n",
    "    drop=True\n",
    ")\n",
    "\n",
    "# Save scores\n",
    "# score_csv = out_dir / \"normalization_scores.csv\"\n",
    "# df_sorted.to_csv(score_csv, index=False)\n",
    "\n",
    "# Display top-5 in console\n",
    "print(\"Top 5 normalization methods by spread score:\")\n",
    "print(df_sorted.head(5).to_string(index=False))\n",
    "\n",
    "# Save best image and histogram\n",
    "best_method = df_sorted.iloc[0][\"method\"]\n",
    "best_img = results[best_method]\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(best_img)\n",
    "plt.title(f\"BEST distributed normalization: {best_method}\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig(str(out_dir / f\"best_{best_method}.png\"), dpi=160)\n",
    "plt.show()\n",
    "\n",
    "plot_hist(best_img, f\"Histogram: BEST = {best_method}\")\n",
    "\n",
    "# # Show the table to the user\n",
    "# from caas_jupyter_tools import display_dataframe_to_user\n",
    "# display_dataframe_to_user(\"Normalization scoring (higher is better)\", df_sorted)\n",
    "\n",
    "print(\"Chosen best method:\", best_method)\n",
    "for name in normalizations:\n",
    "    print(f\" - norm_{name}.png\")\n",
    "    print(f\" - hist_{name}.png\")\n",
    "print(\" - normalization_scores.csv\")\n",
    "print(f\" - best_{best_method}.png\")\n",
    "print(f\" - hist_best_{best_method}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc46bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best haadf after normalization\n",
    "haadf_normalized = best_img\n",
    "plt.imshow(haadf_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ab2cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get spectrum at bunch of points\n",
    "N = 20\n",
    "\n",
    "xs = rng.integers(0, W, size=N)\n",
    "ys = rng.integers(0, H, size=N)\n",
    "pixel_pos = np.stack([xs, ys], axis=1)\n",
    "# Generate n 2D points (n rows, 2 columns) with values between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18117836",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_arrays = []  # 1. Create an empty list before the loop\n",
    "for point in pixel_pos:\n",
    "    x_pos = point[0] * W\n",
    "    y_pos = point[1] * H\n",
    "\n",
    "    # get random array of length 2048\n",
    "    mean = 0.0  # Mean of the distribution\n",
    "    std_dev = 1.0  # Standard deviation of the distribution\n",
    "    array_length = 2048  # Desired number of samples\n",
    "    random_gaussian_array = np.random.normal(loc=mean, scale=std_dev, size=array_length)\n",
    "\n",
    "    # stack the arrays\n",
    "    all_arrays.append(random_gaussian_array)  # 2. Add the new array to the list\n",
    "\n",
    "spectra = np.stack(all_arrays, axis=0)\n",
    "print(spectra.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac84e4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: normalize spectra (safe min-max per spectrum)\n",
    "mins = spectra.min(axis=1, keepdims=True)\n",
    "ptps = np.ptp(spectra, axis=1, keepdims=True)\n",
    "ptps = np.where(ptps == 0, 1.0, ptps)\n",
    "spectra_norm = (spectra - mins) / ptps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a711ed9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) PCA\n",
    "k = 3\n",
    "pca = PCA(n_components=k, random_state=42)\n",
    "scores = pca.fit_transform(spectra_norm)  # shape (N, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd32758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 5) Plots\n",
    "# -----------------------------\n",
    "\n",
    "# a) HAADF overlay with PC1 color\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(haadf_normalized)\n",
    "plt.scatter(xs, ys, c=scores[:, 0], s=50)\n",
    "plt.title(\"HAADF + EDX PCA Overlay (PC1 color)\")\n",
    "plt.colorbar(label=\"PC1 score\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"/mnt/data/overlay_plot.png\", dpi=160)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5745a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMeans on PCA scores\n",
    "km = KMeans(n_clusters=3, n_init=10, random_state=42)\n",
    "cluster_labels = km.fit_predict(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8fb24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) PCA scatter with clusters\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(scores[:, 0], scores[:, 1], s=8, alpha=0.8, c=cluster_labels)\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"PCA Scatter of EDX Spectra with KMeans Clusters\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"/mnt/data/pca_scatter.png\", dpi=160)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38a2117",
   "metadata": {},
   "source": [
    "## 3. Live Mic with EDX\n",
    "- save data on disk:\n",
    "    - haadf:\n",
    "        - raw\n",
    "        - normalized\n",
    "    \n",
    "    - spectrum:\n",
    "        - spectra before normlaization\n",
    "        - spectra after normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50a2adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stemOrchestrator.logging_config import setup_logging\n",
    "\n",
    "data_folder = \".\"\n",
    "out_path = data_folder\n",
    "setup_logging(out_path=out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ce40c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stemOrchestrator.acquisition import TFacquisition, DMacquisition\n",
    "from stemOrchestrator.simulation import DMtwin\n",
    "from stemOrchestrator.process import HAADF_tiff_to_png, tiff_to_png\n",
    "from autoscript_tem_microscope_client import TemMicroscopeClient\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "plot = plt\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640bba1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "ip = os.getenv(\"MICROSCOPE_IP\")\n",
    "port = os.getenv(\"MICROSCOPE_PORT\")\n",
    "\n",
    "if not ip or not port:\n",
    "    secret_path = Path(\"../../config_secret.json\")\n",
    "    if secret_path.exists():\n",
    "        with open(secret_path, \"r\") as f:\n",
    "            secret = json.load(f)\n",
    "            ip = ip or secret.get(\"ip_TF\")\n",
    "            port = port or secret.get(\"port_TF\")\n",
    "\n",
    "\n",
    "if ip is None:\n",
    "    print(\"please check path of yaml file containing ip and port info\")\n",
    "\n",
    "else:\n",
    "    print(\"your yaml file with ip and port loaded fine\")\n",
    "config = {\n",
    "    \"ip\": ip,\n",
    "    \"port\": port,\n",
    "    \"haadf_exposure\": 40e-8,  # micro-seconds per pixel\n",
    "    \"haadf_resolution\": 512,  # square\n",
    "    \"out_path\": \".\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b529d9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ip = config[\"ip\"]\n",
    "port = config[\"port\"]\n",
    "haadf_exposure = config[\"haadf_exposure\"]\n",
    "out_path = config[\"out_path\"]\n",
    "haadf_resolution = config[\"haadf_resolution\"]\n",
    "\n",
    "\n",
    "microscope = TemMicroscopeClient()\n",
    "microscope.connect(ip, port=port)  # 7521 on velox  computer\n",
    "# microscope.connect( port = port)# 7521 on velox  computer\n",
    "\n",
    "# query state:\n",
    "\n",
    "tf_acquisition = TFacquisition(microscope=microscope)\n",
    "\n",
    "# put beam shift to 0,0\n",
    "# tf_acquisition.move_beam_shift_positon([0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5afc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get haadf from mic\n",
    "\n",
    "# Get haadf\n",
    "haadf_np_array, haadf_tiff_name = tf_acquisition.acquire_haadf(\n",
    "    exposure=haadf_exposure, resolution=haadf_resolution\n",
    ")\n",
    "\n",
    "HAADF_tiff_to_png(haadf_tiff_name)\n",
    "haadf = haadf_np_array\n",
    "\n",
    "W, H = haadf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adacf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # normalize haadf\n",
    "\n",
    "# # Show the HAADF image\n",
    "\n",
    "# import pandas as pd\n",
    "# from skimage import exposure\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(6, 6))\n",
    "# plt.imshow(haadf)\n",
    "# plt.title(\"Synthetic HAADF (512x512)\")\n",
    "# plt.tight_layout()\n",
    "# # plt.savefig(\"/mnt/data/haadf.png\", dpi=160)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# # -----------------------------\n",
    "# # b) Plot pixel distribution\n",
    "# # -----------------------------\n",
    "# def plot_hist(arr, title):\n",
    "#     arr_flat = arr.ravel()\n",
    "#     plt.figure(figsize=(6, 4))\n",
    "#     plt.hist(arr_flat, bins=100)\n",
    "#     plt.title(title)\n",
    "#     plt.xlabel(\"Pixel value\")\n",
    "#     plt.ylabel(\"Count\")\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# plot_hist(haadf, \"Histogram: Original HAADF\")\n",
    "\n",
    "\n",
    "# # -----------------------------\n",
    "# # c) Normalizations\n",
    "# # -----------------------------\n",
    "# def safe_minmax(a):\n",
    "#     a = a.astype(float)\n",
    "#     mn = np.nanmin(a)\n",
    "#     ptp = np.ptp(a)\n",
    "#     ptp = ptp if ptp != 0 else 1.0\n",
    "#     return (a - mn) / ptp\n",
    "\n",
    "\n",
    "# def zscore_rescaled(a):\n",
    "#     a = a.astype(float)\n",
    "#     mu = np.nanmean(a)\n",
    "#     sd = np.nanstd(a)\n",
    "#     sd = sd if sd != 0 else 1.0\n",
    "#     z = (a - mu) / sd\n",
    "#     return safe_minmax(z)\n",
    "\n",
    "\n",
    "# def robust_zscore_rescaled(a):\n",
    "#     a = a.astype(float)\n",
    "#     med = np.nanmedian(a)\n",
    "#     mad = np.nanmedian(np.abs(a - med))\n",
    "#     scale = 1.4826 * mad if mad != 0 else 1.0\n",
    "#     z = (a - med) / scale\n",
    "#     return safe_minmax(z)\n",
    "\n",
    "\n",
    "# def percentile_minmax(a, lo=1, hi=99):\n",
    "#     a = a.astype(float)\n",
    "#     p1, p99 = np.nanpercentile(a, [lo, hi])\n",
    "#     denom = max(p99 - p1, 1e-9)\n",
    "#     return np.clip((a - p1) / denom, 0, 1)\n",
    "\n",
    "\n",
    "# def hist_equalize(a):\n",
    "#     a = safe_minmax(a)\n",
    "#     return exposure.equalize_hist(a)\n",
    "\n",
    "\n",
    "# def clahe(a, clip=0.01):\n",
    "#     a = safe_minmax(a)\n",
    "#     return exposure.equalize_adapthist(a, clip_limit=clip)\n",
    "\n",
    "\n",
    "# def background_subtract(a, sigma=20):\n",
    "#     a = a.astype(float)\n",
    "#     bg = ndi.gaussian_filter(a, sigma=sigma)\n",
    "#     res = a - bg\n",
    "#     return safe_minmax(res)\n",
    "\n",
    "\n",
    "# def log_compress(a, gain=5.0):\n",
    "#     a = safe_minmax(a)\n",
    "#     return np.log1p(gain * a) / np.log1p(gain)\n",
    "\n",
    "\n",
    "# def gamma_correct(a, gamma=0.6):\n",
    "#     a = safe_minmax(a)\n",
    "#     return np.power(a, gamma)\n",
    "\n",
    "\n",
    "# def lcn(a, size=15):\n",
    "#     a = a.astype(float)\n",
    "#     mu = ndi.uniform_filter(a, size=size)\n",
    "#     sd = np.sqrt(ndi.uniform_filter((a - mu) ** 2, size=size)) + 1e-6\n",
    "#     z = (a - mu) / sd\n",
    "#     return safe_minmax(z)\n",
    "\n",
    "\n",
    "# # Simulate a flat-field (gain) pattern and correct\n",
    "# def flat_field_correct(a):\n",
    "#     a = a.astype(float)\n",
    "#     # synthetic spatial sensitivity\n",
    "#     yy, xx = np.mgrid[0 : a.shape[0], 0 : a.shape[1]]\n",
    "#     sens = 0.8 + 0.2 * (np.sin(xx / 40.0) * np.cos(yy / 50.0))\n",
    "#     dark = 0.02  # small offset\n",
    "#     corrected = (a - dark) / np.maximum(sens, 1e-6)\n",
    "#     return safe_minmax(corrected)\n",
    "\n",
    "\n",
    "# normalizations = {\n",
    "#     \"minmax\": safe_minmax,\n",
    "#     \"zscore_rescaled\": zscore_rescaled,\n",
    "#     \"robust_zscore_rescaled\": robust_zscore_rescaled,\n",
    "#     \"percentile_minmax_1_99\": lambda a: percentile_minmax(a, 1, 99),\n",
    "#     \"hist_equalize\": hist_equalize,\n",
    "#     \"clahe\": clahe,\n",
    "#     \"background_subtract_sigma20\": background_subtract,\n",
    "#     \"log_compress_gain5\": log_compress,\n",
    "#     \"gamma_0.6\": gamma_correct,\n",
    "#     \"lcn_size15\": lcn,\n",
    "#     \"flat_field_correct\": flat_field_correct,\n",
    "# }\n",
    "\n",
    "# results = {}\n",
    "# # out_dir = Path(\"/mnt/data\")\n",
    "# # out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# # Apply each normalization, plot histogram and save normalized image\n",
    "# for name, func in normalizations.items():\n",
    "#     try:\n",
    "#         arr = func(haadf)\n",
    "#         arr = np.nan_to_num(arr, nan=0.0, posinf=1.0, neginf=0.0)\n",
    "#         arr = np.clip(arr, 0, 1)  # ensure comparable domain\n",
    "#         results[name] = arr\n",
    "\n",
    "#         # Save image\n",
    "#         plt.figure(figsize=(6, 6))\n",
    "#         plt.imshow(arr)\n",
    "#         plt.title(f\"Normalized: {name}\")\n",
    "#         plt.tight_layout()\n",
    "#         # plt.savefig(str(out_dir / f\"norm_{name}.png\"), dpi=160)\n",
    "#         plt.show()\n",
    "\n",
    "#         # Plot histogram\n",
    "#         plot_hist(arr, f\"Histogram: {name}\")\n",
    "#     except Exception as e:\n",
    "#         # record failure\n",
    "#         results[name] = None\n",
    "#         print(f\"[WARN] {name} failed with error: {e}\")\n",
    "\n",
    "\n",
    "# # -----------------------------\n",
    "# # d) Score and choose best-distributed\n",
    "# # -----------------------------\n",
    "# def histogram_entropy(a, bins=256):\n",
    "#     h, edges = np.histogram(a.ravel(), bins=bins, range=(0, 1), density=True)\n",
    "#     h = h + 1e-12  # avoid log(0)\n",
    "#     p = h / h.sum()\n",
    "#     return -np.sum(p * np.log(p))\n",
    "\n",
    "\n",
    "# def spread_score(a):\n",
    "#     # Combined score: entropy + normalized std\n",
    "#     ent = histogram_entropy(a, bins=256)\n",
    "#     std = float(np.std(a))\n",
    "#     return ent + std, ent, std\n",
    "\n",
    "\n",
    "# rows = []\n",
    "# for name, arr in results.items():\n",
    "#     if arr is None:\n",
    "#         rows.append((name, np.nan, np.nan, np.nan))\n",
    "#     else:\n",
    "#         total, ent, std = spread_score(arr)\n",
    "#         rows.append((name, total, ent, std))\n",
    "\n",
    "# df = pd.DataFrame(rows, columns=[\"method\", \"score_total(ent+std)\", \"entropy\", \"std\"])\n",
    "# df_sorted = df.sort_values(\"score_total(ent+std)\", ascending=False).reset_index(\n",
    "#     drop=True\n",
    "# )\n",
    "\n",
    "# # Save scores\n",
    "# # score_csv = out_dir / \"normalization_scores.csv\"\n",
    "# # df_sorted.to_csv(score_csv, index=False)\n",
    "\n",
    "# # Display top-5 in console\n",
    "# print(\"Top 5 normalization methods by spread score:\")\n",
    "# print(df_sorted.head(5).to_string(index=False))\n",
    "\n",
    "# # Save best image and histogram\n",
    "# best_method = df_sorted.iloc[0][\"method\"]\n",
    "# best_img = results[best_method]\n",
    "# plt.figure(figsize=(6, 6))\n",
    "# plt.imshow(best_img)\n",
    "# plt.title(f\"BEST distributed normalization: {best_method}\")\n",
    "# plt.tight_layout()\n",
    "# # plt.savefig(str(out_dir / f\"best_{best_method}.png\"), dpi=160)\n",
    "# plt.show()\n",
    "\n",
    "# plot_hist(best_img, f\"Histogram: BEST = {best_method}\")\n",
    "\n",
    "# # # Show the table to the user\n",
    "# # from caas_jupyter_tools import display_dataframe_to_user\n",
    "# # display_dataframe_to_user(\"Normalization scoring (higher is better)\", df_sorted)\n",
    "\n",
    "# print(\"Chosen best method:\", best_method)\n",
    "# for name in normalizations:\n",
    "#     print(f\" - norm_{name}.png\")\n",
    "#     print(f\" - hist_{name}.png\")\n",
    "# print(\" - normalization_scores.csv\")\n",
    "# print(f\" - best_{best_method}.png\")\n",
    "# print(f\" - hist_best_{best_method}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40648e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best haadf after normalization\n",
    "haadf_normalized = haadf_np_array\n",
    "\n",
    "plt.imshow(haadf_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe93a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds positions to sample from\n",
    "N = 20\n",
    "rng = np.random.default_rng(42)\n",
    "xs = rng.integers(0, W, size=N)\n",
    "ys = rng.integers(0, H, size=N)\n",
    "pixel_pos = np.stack([xs, ys], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9016a804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot position sampled\n",
    "plt.imshow(haadf_normalized)\n",
    "plt.scatter(xs, ys, s=50, c=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba2ed63",
   "metadata": {},
   "outputs": [],
   "source": [
    "## setup edx acquire\n",
    "import xmltodict\n",
    "import json\n",
    "from autoscript_tem_microscope_client.structures import EdsAcquisitionSettings\n",
    "from autoscript_tem_microscope_client.enumerations import (\n",
    "    EdsDetectorType,\n",
    "    ExposureTimeType,\n",
    ")\n",
    "\n",
    "\n",
    "def get_channel_index(energy_keV: float, dispersion: float, offset: float) -> int:\n",
    "    \"\"\"Convert energy (keV) into spectrum channel index.\"\"\"\n",
    "    return int(round((energy_keV - offset) / dispersion))\n",
    "\n",
    "\n",
    "def get_dispersion_and_offset(spectrum):\n",
    "    \"\"\"\n",
    "    Extract dispersion and offset from EDS spectrum metadata (xml).\n",
    "    Returns (dispersion_keV_per_ch, offset_keV).\n",
    "    \"\"\"\n",
    "    xml_string = spectrum.metadata.metadata_as_xml\n",
    "    metadata = xmltodict.parse(xml_string)\n",
    "    metadata = json.loads(json.dumps(metadata))\n",
    "\n",
    "    detectors = metadata[\"Metadata\"][\"Detectors\"][\"AnalyticalDetector\"]\n",
    "\n",
    "    # If only one detector, wrap it into a list\n",
    "    if isinstance(detectors, dict):\n",
    "        detectors = [detectors]\n",
    "\n",
    "    # Take the first detector (or filter by name if needed)\n",
    "    det = detectors[0]\n",
    "    dispersion = float(det.get(\"Dispersion\", 0))\n",
    "    offset = float(det.get(\"OffsetEnergy\", 0))\n",
    "\n",
    "    return dispersion, offset\n",
    "\n",
    "\n",
    "def configure_acquisition(exposure_time=2):\n",
    "    \"\"\"Configure the EDS acquisition settings.\"\"\"\n",
    "    # mic_server is global variable intriduced in def run function\n",
    "    eds_detector_name = microscope.detectors.eds_detectors[0]\n",
    "    eds_detector = microscope.detectors.get_eds_detector(eds_detector_name)\n",
    "    # Configure the acquisition\n",
    "    global eds_settings\n",
    "    eds_settings = EdsAcquisitionSettings()\n",
    "    eds_settings.eds_detector = eds_detector_name\n",
    "    eds_settings.dispersion = eds_detector.dispersions[-1]  # 20 keV\n",
    "    eds_settings.shaping_time = eds_detector.shaping_times[-1]\n",
    "    eds_settings.exposure_time = exposure_time\n",
    "    eds_settings.exposure_time_type = ExposureTimeType.LIVE_TIME\n",
    "    return eds_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cfbb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get edx at those positon and stack them\n",
    "edx_exposure = 1  # in seconds\n",
    "eds_settings = configure_acquisition(exposure_time=edx_exposure)\n",
    "\n",
    "all_arrays = []  # 1. Create an empty list before the loop\n",
    "for point in pixel_pos:\n",
    "    # convert to fractional coordinates\n",
    "    x_pos = point[0] / W\n",
    "    y_pos = point[1] / H\n",
    "\n",
    "    # position beam\n",
    "    tf_acquisition.move_paused_beam(x_pos, y_pos)\n",
    "\n",
    "    # Acquire the EDS spectrum\n",
    "    microscope.optics.blanker.unblank()\n",
    "    spectrum = microscope.analysis.eds.acquire_spectrum(eds_settings)\n",
    "    microscope.optics.blanker.blank()\n",
    "\n",
    "    # Average spectrum data from 4 detectors\n",
    "    n_channels_per_detector = len(spectrum.data) // 4\n",
    "    summed_spectrum = np.zeros(n_channels_per_detector)\n",
    "\n",
    "    for i in range(4):\n",
    "        start_idx = i * n_channels_per_detector\n",
    "        end_idx = (i + 1) * n_channels_per_detector\n",
    "        summed_spectrum += spectrum.data[start_idx:end_idx]\n",
    "\n",
    "    # Use summed spectrum for analysis\n",
    "    spectrum_data = summed_spectrum\n",
    "\n",
    "    # stack the arrays\n",
    "    all_arrays.append(spectrum_data)  # 2. Add the new array to the list\n",
    "\n",
    "spectra = np.stack(all_arrays, axis=0)\n",
    "print(spectra.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc12b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the spectra\n",
    "# Optional: normalize spectra (safe min-max per spectrum)\n",
    "mins = spectra.min(axis=1, keepdims=True)\n",
    "ptps = np.ptp(spectra, axis=1, keepdims=True)\n",
    "ptps = np.where(ptps == 0, 1.0, ptps)\n",
    "spectra_norm = (spectra - mins) / ptps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc13a95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca the data\n",
    "# import libraries\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k = 3\n",
    "pca = PCA(n_components=k, random_state=42)\n",
    "scores = pca.fit_transform(spectra_norm)  # shape (N, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2fb3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imshow the pc1 values on the haadf\n",
    "# -----------------------------\n",
    "# 5) Plots\n",
    "# -----------------------------\n",
    "\n",
    "# a) HAADF overlay with PC1 color\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(haadf_normalized, cmap=\"gray\")\n",
    "plt.scatter(xs, ys, c=scores[:, 0], s=50)\n",
    "plt.title(\"HAADF + EDX PCA Overlay (PC1 color)\")\n",
    "plt.colorbar(label=\"PC1 score\", cmap=\"magma\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"/mnt/data/overlay_plot.png\", dpi=160)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c3b2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMeans on PCA scores\n",
    "km = KMeans(n_clusters=3, n_init=10, random_state=42)\n",
    "cluster_labels = km.fit_predict(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad123892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) PCA scatter with clusters\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(scores[:, 0], scores[:, 1], s=8, alpha=0.8, c=cluster_labels)\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"PCA Scatter of EDX Spectra with KMeans Clusters\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"/mnt/data/pca_scatter.png\", dpi=160)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a14f50",
   "metadata": {},
   "source": [
    "## 4. Live -> SAM + EDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ec101d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stemOrchestrator.logging_config import setup_logging\n",
    "import os\n",
    "\n",
    "data_folder = \"out/\"\n",
    "\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "out_path = data_folder\n",
    "setup_logging(out_path=out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8157a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stemOrchestrator.acquisition import TFacquisition, DMacquisition\n",
    "from stemOrchestrator.simulation import DMtwin\n",
    "from stemOrchestrator.process import HAADF_tiff_to_png, tiff_to_png\n",
    "from autoscript_tem_microscope_client import TemMicroscopeClient\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "plot = plt\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feb2741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "ip = os.getenv(\"MICROSCOPE_IP\")\n",
    "port = os.getenv(\"MICROSCOPE_PORT\")\n",
    "\n",
    "if not ip or not port:\n",
    "    secret_path = Path(\"../../config_secret.json\")\n",
    "    if secret_path.exists():\n",
    "        with open(secret_path, \"r\") as f:\n",
    "            secret = json.load(f)\n",
    "            ip = ip or secret.get(\"ip_TF\")\n",
    "            port = port or secret.get(\"port_TF\")\n",
    "\n",
    "\n",
    "if ip is None:\n",
    "    print(\"please check path of yaml file containing ip and port info\")\n",
    "\n",
    "else:\n",
    "    print(\"your yaml file with ip and port loaded fine\")\n",
    "config = {\n",
    "    \"ip\": ip,\n",
    "    \"port\": port,\n",
    "    \"haadf_exposure\": 40e-8,  # micro-seconds per pixel\n",
    "    \"haadf_resolution\": 512,  # square\n",
    "    \"out_path\": f\"{data_folder}\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7a0038",
   "metadata": {},
   "outputs": [],
   "source": [
    "ip = config[\"ip\"]\n",
    "port = config[\"port\"]\n",
    "haadf_exposure = config[\"haadf_exposure\"]\n",
    "out_path = config[\"out_path\"]\n",
    "haadf_resolution = config[\"haadf_resolution\"]\n",
    "\n",
    "\n",
    "microscope = TemMicroscopeClient()\n",
    "microscope.connect(ip, port=port)  # 7521 on velox  computer\n",
    "# microscope.connect( port = port)# 7521 on velox  computer\n",
    "\n",
    "# query state:\n",
    "\n",
    "tf_acquisition = TFacquisition(microscope=microscope)\n",
    "\n",
    "# put beam shift to 0,0\n",
    "# tf_acquisition.move_beam_shift_positon([0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b30cbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get haadf from mic\n",
    "\n",
    "# Get haadf\n",
    "haadf_np_array, haadf_tiff_name = tf_acquisition.acquire_haadf(\n",
    "    exposure=haadf_exposure, resolution=haadf_resolution, folder_path=out_path\n",
    ")\n",
    "\n",
    "HAADF_tiff_to_png(out_path + haadf_tiff_name)\n",
    "haadf = haadf_np_array\n",
    "\n",
    "W, H = haadf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7ca42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best haadf after normalization\n",
    "haadf_normalized = haadf_np_array\n",
    "\n",
    "plt.imshow(haadf_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528edefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get positons from sam\n",
    "\n",
    "########SAM part ********************************************************************************************************\n",
    "# -----> takes quite some time- 3 minutes-- to load --\n",
    "from stemOrchestrator.MLlayer.MLlayerSAM import (\n",
    "    setup_device,\n",
    "    download_sam_model,\n",
    "    initialize_sam_model,\n",
    "    preprocess_image,\n",
    "    generate_and_save_masks,\n",
    "    create_normalized_particle_positions,\n",
    "    display_image_with_masks,\n",
    "    display_image_with_labels,\n",
    "    extract_mask_contours,\n",
    "    generate_mask_colors,\n",
    "    visualize_masks_with_boundaries,\n",
    "    extract_particle_data,\n",
    "    print_boundary_points_info,\n",
    "    plot_centroids,\n",
    "    sample_particle_positions,\n",
    "    plot_sampled_positions,\n",
    "    create_visualization_with_masks,\n",
    ")\n",
    "import pickle\n",
    "import numpy as np\n",
    "from typing import List, Dict, Union\n",
    "\n",
    "\n",
    "def run_sam(image_data: np.ndarray, path_folder: str) -> Union[List, Dict]:\n",
    "    \"\"\"Main function to run SAM segmentation pipeline.\"\"\"\n",
    "    device = setup_device()\n",
    "\n",
    "    model_type = \"vit_b\"  # Options: 'vit_b', 'vit_l', 'vit_h'\n",
    "    checkpoint_url = (\n",
    "        \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\"\n",
    "    )\n",
    "    checkpoint_path = \"sam_vit_b_01ec64.pth\"\n",
    "    download_sam_model(model_type, checkpoint_url, checkpoint_path)\n",
    "    sam, mask_generator = initialize_sam_model(model_type, checkpoint_path, device)\n",
    "    img_np = preprocess_image(image_data)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(img_np)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    # Generate and visualize masks\n",
    "    masks_path = f\"{path_folder}/masks_Au_online.pkl\"\n",
    "    masks = generate_and_save_masks(mask_generator, img_np, masks_path)\n",
    "    visual_image, centroids = create_visualization_with_masks(img_np, masks)\n",
    "    display_image_with_masks(visual_image, \"Image with Segmentation Masks\")\n",
    "    display_image_with_labels(\n",
    "        visual_image, centroids, \"Image with Segmentation Masks and Labels\"\n",
    "    )\n",
    "\n",
    "    mask_contours = extract_mask_contours(masks)\n",
    "    mask_colors = generate_mask_colors(len(masks))\n",
    "    boundaries_path = (\n",
    "        f\"{path_folder}/Segmentation Masks with Boundaries and Centroids.png\"\n",
    "    )\n",
    "    visualize_masks_with_boundaries(\n",
    "        visual_image, centroids, mask_contours, mask_colors, boundaries_path\n",
    "    )\n",
    "    particles = extract_particle_data(masks)\n",
    "    # Save particle data\n",
    "    # with open(f'{path_folder}/particles.pkl', 'wb') as f:\n",
    "    #     pickle.dump(particles, f)\n",
    "\n",
    "    print_boundary_points_info(particles)\n",
    "    centroids_array = np.array(centroids)\n",
    "    plot_centroids(centroids_array, img_np)\n",
    "    positions_sampled = sample_particle_positions(particles, img_np)\n",
    "    plot_sampled_positions(positions_sampled, img_np, len(centroids))\n",
    "    each_particle_position = create_normalized_particle_positions(\n",
    "        particles, img_np.shape[:2]\n",
    "    )\n",
    "    # with open(f'{path_folder}/sampled_boundary_pts_particles.pkl', 'wb') as f: # Save normalized particle positions\n",
    "    #     pickle.dump(each_particle_position, f)\n",
    "\n",
    "    all_particle_keys = each_particle_position.keys()\n",
    "\n",
    "    print(\"Processing complete!\")\n",
    "    return all_particle_keys, each_particle_position\n",
    "\n",
    "\n",
    "##########****************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506db70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run the segmentaiotn on haadf to get particles\n",
    "from datetime import datetime\n",
    "\n",
    "print(datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "all_particle_keys, each_particle_position = run_sam(\n",
    "    haadf_np_array, out_path\n",
    ")  ############ haadf normalized doesnt work here --- weird\n",
    "print(datetime.now().strftime(\"%Y%m%d_%H%M%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3997074f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds positions to sample from\n",
    "# N = 20\n",
    "rng = np.random.default_rng(42)\n",
    "# xs = rng.integers(0, W, size=N)\n",
    "# ys = rng.integers(0, H, size=N)\n",
    "# pixel_pos = np.stack([xs, ys], axis=1)\n",
    "\n",
    "centroids = np.array([v[\"centroid\"] for v in each_particle_position.values()])\n",
    "\n",
    "# Sample N random centroids (without replacement if fewer than N)\n",
    "N = len(centroids)\n",
    "rng = np.random.default_rng(42)\n",
    "idx = rng.choice(len(centroids), size=min(N, len(centroids)), replace=False)\n",
    "sampled = centroids[idx]\n",
    "\n",
    "# Separate xs and ys (keeping same data types as your original code)\n",
    "xs = sampled[:, 0] * W\n",
    "ys = sampled[:, 1] * H\n",
    "pixel_pos = np.stack([xs, ys], axis=1)\n",
    "\n",
    "print(\"xs:\", xs)\n",
    "print(\"ys:\", ys)\n",
    "print(\"pixel_pos:\\n\", pixel_pos)\n",
    "\n",
    "np.save(f\"{out_path}pixel_pos.npy\", pixel_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c1be15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot position sampled\n",
    "plt.imshow(haadf_normalized)\n",
    "plt.scatter(xs, ys, s=50, c=\"r\")\n",
    "# want to save this? - nope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf98ef78",
   "metadata": {},
   "outputs": [],
   "source": [
    "## setup edx acquire\n",
    "import xmltodict\n",
    "import json\n",
    "from autoscript_tem_microscope_client.structures import EdsAcquisitionSettings\n",
    "from autoscript_tem_microscope_client.enumerations import (\n",
    "    EdsDetectorType,\n",
    "    ExposureTimeType,\n",
    ")\n",
    "\n",
    "\n",
    "def get_channel_index(energy_keV: float, dispersion: float, offset: float) -> int:\n",
    "    \"\"\"Convert energy (keV) into spectrum channel index.\"\"\"\n",
    "    return int(round((energy_keV - offset) / dispersion))\n",
    "\n",
    "\n",
    "def get_dispersion_and_offset(spectrum):\n",
    "    \"\"\"\n",
    "    Extract dispersion and offset from EDS spectrum metadata (xml).\n",
    "    Returns (dispersion_keV_per_ch, offset_keV).\n",
    "    \"\"\"\n",
    "    xml_string = spectrum.metadata.metadata_as_xml\n",
    "    metadata = xmltodict.parse(xml_string)\n",
    "    metadata = json.loads(json.dumps(metadata))\n",
    "\n",
    "    detectors = metadata[\"Metadata\"][\"Detectors\"][\"AnalyticalDetector\"]\n",
    "\n",
    "    # If only one detector, wrap it into a list\n",
    "    if isinstance(detectors, dict):\n",
    "        detectors = [detectors]\n",
    "\n",
    "    # Take the first detector (or filter by name if needed)\n",
    "    det = detectors[0]\n",
    "    dispersion = float(det.get(\"Dispersion\", 0))\n",
    "    offset = float(det.get(\"OffsetEnergy\", 0))\n",
    "\n",
    "    return dispersion, offset\n",
    "\n",
    "\n",
    "def configure_acquisition(exposure_time=2):\n",
    "    \"\"\"Configure the EDS acquisition settings.\"\"\"\n",
    "    # mic_server is global variable intriduced in def run function\n",
    "    eds_detector_name = microscope.detectors.eds_detectors[0]\n",
    "    eds_detector = microscope.detectors.get_eds_detector(eds_detector_name)\n",
    "    # Configure the acquisition\n",
    "    global eds_settings\n",
    "    eds_settings = EdsAcquisitionSettings()\n",
    "    eds_settings.eds_detector = eds_detector_name\n",
    "    eds_settings.dispersion = eds_detector.dispersions[-1]  # 20 keV\n",
    "    eds_settings.shaping_time = eds_detector.shaping_times[-1]\n",
    "    eds_settings.exposure_time = exposure_time\n",
    "    eds_settings.exposure_time_type = ExposureTimeType.LIVE_TIME\n",
    "    return eds_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6e76ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get edx at those positon and stack them\n",
    "# 2 minutes for 20 points\n",
    "edx_exposure = 1  # in seconds\n",
    "eds_settings = configure_acquisition(exposure_time=edx_exposure)\n",
    "\n",
    "all_arrays = []  # 1. Create an empty list before the loop\n",
    "for point in pixel_pos:\n",
    "    # convert to fractional coordinates\n",
    "    x_pos = point[0] / W\n",
    "    y_pos = point[1] / H\n",
    "\n",
    "    # position beam\n",
    "    tf_acquisition.move_paused_beam(x_pos, y_pos)\n",
    "\n",
    "    # Acquire the EDS spectrum\n",
    "    microscope.optics.blanker.unblank()\n",
    "    spectrum = microscope.analysis.eds.acquire_spectrum(eds_settings)\n",
    "    microscope.optics.blanker.blank()\n",
    "\n",
    "    # Average spectrum data from 4 detectors\n",
    "    n_channels_per_detector = len(spectrum.data) // 4\n",
    "    summed_spectrum = np.zeros(n_channels_per_detector)\n",
    "\n",
    "    for i in range(4):\n",
    "        start_idx = i * n_channels_per_detector\n",
    "        end_idx = (i + 1) * n_channels_per_detector\n",
    "        summed_spectrum += spectrum.data[start_idx:end_idx]\n",
    "\n",
    "    # Use summed spectrum for analysis\n",
    "    spectrum_data = summed_spectrum\n",
    "\n",
    "    # stack the arrays\n",
    "    all_arrays.append(spectrum_data)  # 2. Add the new array to the list\n",
    "\n",
    "spectra = np.stack(all_arrays, axis=0)\n",
    "print(spectra.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5197ab6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the spectra\n",
    "# Optional: normalize spectra (safe min-max per spectrum)\n",
    "mins = spectra.min(axis=1, keepdims=True)\n",
    "ptps = np.ptp(spectra, axis=1, keepdims=True)\n",
    "ptps = np.where(ptps == 0, 1.0, ptps)\n",
    "spectra_norm = (spectra - mins) / ptps\n",
    "np.save(f\"{out_path}raw_spectra.npy\", spectra)\n",
    "# save raw spectra\n",
    "np.save(f\"{out_path}normalized_spectra.npy\",spectra_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c20d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca the data\n",
    "# import libraries\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k = 3\n",
    "pca = PCA(n_components=k, random_state=42)\n",
    "scores = pca.fit_transform(spectra_norm)  # shape (N, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac44699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imshow the pc1 values on the haadf\n",
    "# -----------------------------\n",
    "# 5) Plots\n",
    "# -----------------------------\n",
    "\n",
    "# a) HAADF overlay with PC1 color\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(haadf_normalized, cmap=\"gray\")\n",
    "plt.scatter(xs, ys, c=scores[:, 1], s=50)\n",
    "plt.title(\"HAADF + EDX PCA Overlay (PC1 color)\")\n",
    "plt.colorbar(label=\"PC1 score\", cmap=\"magma\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"/mnt/data/overlay_plot.png\", dpi=160)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b669ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imshow the pc1 values on the haadf\n",
    "# -----------------------------\n",
    "# 5) Plots\n",
    "# -----------------------------\n",
    "\n",
    "# a) HAADF overlay with PC1 color\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(haadf_normalized, cmap=\"gray\")\n",
    "plt.scatter(xs, ys, c=scores[:, 1], s=50)\n",
    "plt.title(\"HAADF + EDX PCA Overlay (PC2 color)\")\n",
    "plt.colorbar(label=\"PC1 score\", cmap=\"magma\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"/mnt/data/overlay_plot.png\", dpi=160)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642dfc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMeans on PCA scores\n",
    "km = KMeans(n_clusters=3, n_init=10, random_state=42)\n",
    "cluster_labels = km.fit_predict(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9909c158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) PCA scatter with clusters\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(scores[:, 0], scores[:, 1], s=8, alpha=0.8, c=cluster_labels)\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"PCA Scatter of EDX Spectra with KMeans Clusters\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"/mnt/data/pca_scatter.png\", dpi=160)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8f2c22",
   "metadata": {},
   "source": [
    "## end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1167cbd5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stemorchestrator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
