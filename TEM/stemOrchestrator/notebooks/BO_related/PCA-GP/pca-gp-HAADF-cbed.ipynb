{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02538a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import logging\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# For offline mode simulation\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5626035a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Initialize Microscope\n",
    "from stemOrchestrator.acquisition import TFacquisition, DMacquisition\n",
    "from autoscript_tem_microscope_client import TemMicroscopeClient\n",
    "\n",
    "# Choose your mode\n",
    "OFFLINE_MODE = True  # Set to False for real microscope operation\n",
    "\n",
    "if OFFLINE_MODE:\n",
    "    # Offline simulation mode\n",
    "    print(\"Running in OFFLINE simulation mode\")\n",
    "    microscope = None\n",
    "    acquisition = None#TFacquisition(microscope, offline=True)\n",
    "else:\n",
    "    # Real microscope operation\n",
    "    print(\"Connecting to real microscope...\")\n",
    "    microscope = TemMicroscopeClient()\n",
    "    microscope.connect(\"localhost\")  # Adjust IP as needed\n",
    "    acquisition = TFacquisition(microscope, offline=False)\n",
    "\n",
    "print(\"Microscope initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cd9c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Simulation Functions for Offline Mode\n",
    "def simulate_haadf_image(size: int = 512) -> np.ndarray:\n",
    "    \"\"\"Simulate a HAADF image with various features\"\"\"\n",
    "    # Create base noise\n",
    "    image = np.random.normal(0, 0.1, (size, size))\n",
    "    \n",
    "    # Add some circular features (nanoparticles)\n",
    "    for _ in range(10):\n",
    "        x, y = np.random.randint(50, size-50, 2)\n",
    "        radius = np.random.randint(10, 30)\n",
    "        intensity = np.random.uniform(0.5, 2.0)\n",
    "        \n",
    "        yy, xx = np.ogrid[:size, :size]\n",
    "        mask = (xx - x)**2 + (yy - y)**2 <= radius**2\n",
    "        image[mask] += intensity\n",
    "    \n",
    "    # Add some linear features\n",
    "    for _ in range(5):\n",
    "        x1, y1 = np.random.randint(0, size, 2)\n",
    "        x2, y2 = np.random.randint(0, size, 2)\n",
    "        thickness = np.random.randint(2, 8)\n",
    "        intensity = np.random.uniform(0.3, 1.0)\n",
    "        \n",
    "        # Simple line drawing\n",
    "        length = int(np.sqrt((x2-x1)**2 + (y2-y1)**2))\n",
    "        if length > 0:\n",
    "            xs = np.linspace(x1, x2, length).astype(int)\n",
    "            ys = np.linspace(y1, y2, length).astype(int)\n",
    "            for i in range(-thickness//2, thickness//2):\n",
    "                for j in range(-thickness//2, thickness//2):\n",
    "                    valid_x = np.clip(xs + i, 0, size-1)\n",
    "                    valid_y = np.clip(ys + j, 0, size-1)\n",
    "                    image[valid_y, valid_x] += intensity\n",
    "    \n",
    "    # Normalize to 0-255\n",
    "    image = (image - image.min()) / (image.max() - image.min())\n",
    "    return (image * 255).astype(np.uint8)\n",
    "\n",
    "def simulate_cbed_pattern(patch: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Simulate CBED pattern based on patch characteristics\"\"\"\n",
    "    # Extract features from patch\n",
    "    mean_intensity = np.mean(patch)\n",
    "    std_intensity = np.std(patch)\n",
    "    \n",
    "    # Create base diffraction pattern (256x256)\n",
    "    cbed = np.zeros((256, 256))\n",
    "    center = 128\n",
    "    \n",
    "    # Central beam\n",
    "    y, x = np.ogrid[:256, :256]\n",
    "    central_mask = (x - center)**2 + (y - center)**2 <= 25\n",
    "    cbed[central_mask] = 1000 * (1 + mean_intensity/255.0)\n",
    "    \n",
    "    # Add diffraction spots based on patch characteristics\n",
    "    num_spots = int(10 * (std_intensity / 50.0))  # More spots for higher variance\n",
    "    for i in range(num_spots):\n",
    "        angle = np.random.uniform(0, 2*np.pi)\n",
    "        distance = np.random.uniform(40, 100)\n",
    "        spot_x = int(center + distance * np.cos(angle))\n",
    "        spot_y = int(center + distance * np.sin(angle))\n",
    "        \n",
    "        if 10 < spot_x < 246 and 10 < spot_y < 246:\n",
    "            intensity = np.random.uniform(50, 200) * (mean_intensity/255.0)\n",
    "            spot_mask = (x - spot_x)**2 + (y - spot_y)**2 <= 9\n",
    "            cbed[spot_mask] += intensity\n",
    "    \n",
    "    # Add noise\n",
    "    cbed += np.random.normal(0, 10, cbed.shape)\n",
    "    cbed = np.maximum(cbed, 0)  # No negative intensities\n",
    "    \n",
    "    return cbed.astype(np.float32)\n",
    "\n",
    "def extract_scalar_from_cbed(cbed: np.ndarray) -> float:\n",
    "    \"\"\"Extract a scalar property from CBED pattern\"\"\"\n",
    "    # This could represent crystallinity, strain, or other structural property\n",
    "    # Using a combination of features\n",
    "    \n",
    "    # Feature 1: Intensity ratio (spots vs background)\n",
    "    center_region = cbed[108:148, 108:148]  # Central 40x40 region\n",
    "    outer_region = cbed[50:206, 50:206]     # Outer region\n",
    "    outer_region = outer_region[~np.isin(np.arange(156*156), \n",
    "                                       np.arange(58*58) + 49*156 + 49).reshape(156, 156)]\n",
    "    \n",
    "    intensity_ratio = np.mean(center_region) / (np.mean(outer_region) + 1e-6)\n",
    "    \n",
    "    # Feature 2: Symmetry measure\n",
    "    flipped_cbed = np.fliplr(cbed)\n",
    "    symmetry = 1.0 / (1.0 + np.mean((cbed - flipped_cbed)**2))\n",
    "    \n",
    "    # Feature 3: High frequency content\n",
    "    fft_cbed = np.fft.fft2(cbed)\n",
    "    high_freq = np.sum(np.abs(fft_cbed[64:192, 64:192])) / np.sum(np.abs(fft_cbed))\n",
    "    \n",
    "    # Combine features into a single scalar (representing some material property)\n",
    "    scalar_property = 0.4 * intensity_ratio + 0.3 * symmetry + 0.3 * high_freq\n",
    "    \n",
    "    return float(scalar_property)\n",
    "\n",
    "print(\"Simulation functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8088f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Active Learning Class Implementation\n",
    "class ActiveLearningSTEM:\n",
    "    def __init__(self, acquisition_system, patch_size: int = 64, n_pca_components: int = 10):\n",
    "        self.acquisition = acquisition_system\n",
    "        self.patch_size = patch_size\n",
    "        self.n_pca_components = n_pca_components\n",
    "        \n",
    "        # Initialize PCA and scaler\n",
    "        self.pca = PCA(n_components=n_pca_components)\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "        # Initialize Gaussian Process\n",
    "        kernel = RBF(length_scale=1.0) + WhiteKernel(noise_level=0.1)\n",
    "        self.gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, \n",
    "                                         normalize_y=True, random_state=42)\n",
    "        \n",
    "        # Storage for data\n",
    "        self.patches = []\n",
    "        self.patch_coordinates = []\n",
    "        self.cbed_patterns = []\n",
    "        self.scalar_properties = []\n",
    "        self.pca_features = None\n",
    "        \n",
    "        print(f\"Active Learning system initialized with patch size {patch_size}x{patch_size}\")\n",
    "    \n",
    "    def get_haadf_image(self) -> np.ndarray:\n",
    "        \"\"\"Acquire HAADF image\"\"\"\n",
    "        if OFFLINE_MODE:\n",
    "            return simulate_haadf_image(512)\n",
    "        else:\n",
    "            image_data, _ = self.acquisition.acquire_haadf(exposure=40e-9, resolution=512)\n",
    "            return image_data\n",
    "    \n",
    "    def extract_patches(self, image: np.ndarray, stride: Optional[int] = None) -> List[Dict]:\n",
    "        \"\"\"Extract patches from HAADF image\"\"\"\n",
    "        if stride is None:\n",
    "            stride = self.patch_size // 2\n",
    "        \n",
    "        patches_info = []\n",
    "        h, w = image.shape\n",
    "        \n",
    "        for y in range(0, h - self.patch_size + 1, stride):\n",
    "            for x in range(0, w - self.patch_size + 1, stride):\n",
    "                patch = image[y:y+self.patch_size, x:x+self.patch_size]\n",
    "                \n",
    "                patches_info.append({\n",
    "                    'patch': patch,\n",
    "                    'coordinates': (x + self.patch_size//2, y + self.patch_size//2),\n",
    "                    'index': len(patches_info)\n",
    "                })\n",
    "        \n",
    "        print(f\"Extracted {len(patches_info)} patches from image\")\n",
    "        return patches_info\n",
    "    \n",
    "    def get_cbed_pattern(self, coordinates: Tuple[int, int]) -> np.ndarray:\n",
    "        \"\"\"Acquire CBED pattern at given coordinates\"\"\"\n",
    "        if OFFLINE_MODE:\n",
    "            # Get the patch at these coordinates for simulation\n",
    "            patch_idx = None\n",
    "            for i, coord in enumerate(self.patch_coordinates):\n",
    "                if coord == coordinates:\n",
    "                    patch_idx = i\n",
    "                    break\n",
    "            \n",
    "            if patch_idx is not None and patch_idx < len(self.patches):\n",
    "                return simulate_cbed_pattern(self.patches[patch_idx])\n",
    "            else:\n",
    "                # Create dummy patch for simulation\n",
    "                dummy_patch = np.random.randint(0, 255, (self.patch_size, self.patch_size))\n",
    "                return simulate_cbed_pattern(dummy_patch)\n",
    "        else:\n",
    "            # Real microscope operation\n",
    "            # Move beam to coordinates and acquire CBED\n",
    "            x_norm = coordinates[0] / 512.0\n",
    "            y_norm = coordinates[1] / 512.0\n",
    "            self.acquisition.move_paused_beam(x_norm, y_norm)\n",
    "            cbed_data, _ = self.acquisition.acquire_ceta_or_flucam(\n",
    "                exposure=0.1, resolution=256, camera=\"ceta\"\n",
    "            )\n",
    "            return cbed_data\n",
    "    \n",
    "    def compute_pca_features(self, patches: List[np.ndarray]) -> np.ndarray:\n",
    "        \"\"\"Compute PCA features for patches\"\"\"\n",
    "        # Flatten patches\n",
    "        flattened_patches = np.array([patch.flatten() for patch in patches])\n",
    "        \n",
    "        # Scale features\n",
    "        scaled_patches = self.scaler.fit_transform(flattened_patches)\n",
    "        \n",
    "        # Apply PCA\n",
    "        pca_features = self.pca.fit_transform(scaled_patches)\n",
    "        \n",
    "        print(f\"PCA explained variance ratio: {self.pca.explained_variance_ratio_}\")\n",
    "        print(f\"Total explained variance: {np.sum(self.pca.explained_variance_ratio_):.3f}\")\n",
    "        \n",
    "        return pca_features\n",
    "    \n",
    "    def acquisition_function(self, X_train: np.ndarray, X_candidates: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Upper Confidence Bound acquisition function\"\"\"\n",
    "        if len(X_train) == 0:\n",
    "            # Random selection for first point\n",
    "            return np.random.random(len(X_candidates))\n",
    "        \n",
    "        # Predict mean and variance\n",
    "        mean, std = self.gp.predict(X_candidates, return_std=True)\n",
    "        \n",
    "        # UCB with beta=2\n",
    "        ucb = mean + 2.0 * std\n",
    "        \n",
    "        return ucb\n",
    "    \n",
    "    def select_next_measurement(self, candidate_features: np.ndarray, \n",
    "                              measured_indices: List[int]) -> int:\n",
    "        \"\"\"Select next measurement location using acquisition function\"\"\"\n",
    "        if not measured_indices:\n",
    "            # Random first selection\n",
    "            return np.random.randint(len(candidate_features))\n",
    "        \n",
    "        # Get unmeasured candidates\n",
    "        unmeasured_indices = [i for i in range(len(candidate_features)) \n",
    "                            if i not in measured_indices]\n",
    "        \n",
    "        if not unmeasured_indices:\n",
    "            return None\n",
    "        \n",
    "        # Compute acquisition function values\n",
    "        measured_features = candidate_features[measured_indices]\n",
    "        candidate_features_unmeasured = candidate_features[unmeasured_indices]\n",
    "        \n",
    "        acquisition_values = self.acquisition_function(measured_features, \n",
    "                                                     candidate_features_unmeasured)\n",
    "        \n",
    "        # Select best candidate\n",
    "        best_candidate_idx = np.argmax(acquisition_values)\n",
    "        return unmeasured_indices[best_candidate_idx]\n",
    "    \n",
    "    def train_gp_model(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"Train Gaussian Process model\"\"\"\n",
    "        self.gp.fit(X, y)\n",
    "        score = self.gp.score(X, y)\n",
    "        print(f\"GP model trained. R² score: {score:.3f}\")\n",
    "    \n",
    "    def predict_properties(self, X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Predict properties using trained GP model\"\"\"\n",
    "        return self.gp.predict(X, return_std=True)\n",
    "\n",
    "print(\"Active Learning class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90fc34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Initialize Active Learning System and Acquire Initial Data\n",
    "# Initialize the active learning system\n",
    "al_system = ActiveLearningSTEM(acquisition, patch_size=64, n_pca_components=10)\n",
    "\n",
    "# Step 1: Acquire HAADF image\n",
    "print(\"Step 1: Acquiring HAADF image...\")\n",
    "haadf_image = al_system.get_haadf_image()\n",
    "\n",
    "# Display the image\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(haadf_image, cmap='gray')\n",
    "plt.title('HAADF Image')\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(f\"HAADF image acquired with shape: {haadf_image.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbba93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Extract Patches and Compute Initial Features\n",
    "# Step 2: Extract patches\n",
    "print(\"\\nStep 2: Extracting patches...\")\n",
    "patches_info = al_system.extract_patches(haadf_image, stride=32)\n",
    "\n",
    "# Store patch data\n",
    "al_system.patches = [info['patch'] for info in patches_info]\n",
    "al_system.patch_coordinates = [info['coordinates'] for info in patches_info]\n",
    "\n",
    "# Step 3: Compute PCA features for all patches\n",
    "print(\"\\nStep 3: Computing PCA features...\")\n",
    "al_system.pca_features = al_system.compute_pca_features(al_system.patches)\n",
    "\n",
    "# Visualize some patches\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "for i in range(10):\n",
    "    ax = axes[i//5, i%5]\n",
    "    ax.imshow(al_system.patches[i], cmap='gray')\n",
    "    ax.set_title(f'Patch {i}\\n@{al_system.patch_coordinates[i]}')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total patches extracted: {len(al_system.patches)}\")\n",
    "print(f\"PCA features shape: {al_system.pca_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e903e2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Initial Measurements (Bootstrap)\n",
    "# Step 4: Initial bootstrap measurements (10 random patches)\n",
    "print(\"\\nStep 4: Initial bootstrap measurements...\")\n",
    "\n",
    "n_initial = 10\n",
    "initial_indices = np.random.choice(len(al_system.patches), n_initial, replace=False)\n",
    "\n",
    "measured_indices = []\n",
    "measured_cbed_patterns = []\n",
    "measured_scalar_properties = []\n",
    "\n",
    "for i, patch_idx in enumerate(initial_indices):\n",
    "    print(f\"Initial measurement {i+1}/{n_initial}: Patch {patch_idx}\")\n",
    "    \n",
    "    coordinates = al_system.patch_coordinates[patch_idx]\n",
    "    \n",
    "    # Acquire CBED pattern\n",
    "    cbed_pattern = al_system.get_cbed_pattern(coordinates)\n",
    "    \n",
    "    # Extract scalar property\n",
    "    scalar_property = extract_scalar_from_cbed(cbed_pattern)\n",
    "    \n",
    "    # Store measurements\n",
    "    measured_indices.append(patch_idx)\n",
    "    measured_cbed_patterns.append(cbed_pattern)\n",
    "    measured_scalar_properties.append(scalar_property)\n",
    "    \n",
    "    al_system.cbed_patterns.append(cbed_pattern)\n",
    "    al_system.scalar_properties.append(scalar_property)\n",
    "\n",
    "print(f\"Initial measurements completed!\")\n",
    "print(f\"Scalar properties range: {np.min(measured_scalar_properties):.3f} - {np.max(measured_scalar_properties):.3f}\")\n",
    "\n",
    "# Visualize initial measurements\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "for i in range(10):\n",
    "    # Show patch\n",
    "    ax1 = axes[0, i]\n",
    "    patch_idx = measured_indices[i]\n",
    "    ax1.imshow(al_system.patches[patch_idx], cmap='gray')\n",
    "    ax1.set_title(f'Patch {patch_idx}\\nProperty: {measured_scalar_properties[i]:.3f}')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Show CBED\n",
    "    ax2 = axes[1, i]\n",
    "    ax2.imshow(measured_cbed_patterns[i], cmap='hot')\n",
    "    ax2.set_title('CBED Pattern')\n",
    "    ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bdd0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Train Initial GP Model\n",
    "# Step 5: Train initial GP model\n",
    "print(\"\\nStep 5: Training initial GP model...\")\n",
    "\n",
    "# Get features for measured patches\n",
    "X_measured = al_system.pca_features[measured_indices]\n",
    "y_measured = np.array(measured_scalar_properties)\n",
    "\n",
    "# Train GP model\n",
    "al_system.train_gp_model(X_measured, y_measured)\n",
    "\n",
    "# Make predictions on all patches\n",
    "y_pred_all, y_std_all = al_system.predict_properties(al_system.pca_features)\n",
    "\n",
    "# Visualize initial predictions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot 1: Measured vs Predicted (for measured points)\n",
    "axes[0].scatter(y_measured, y_pred_all[measured_indices], alpha=0.7)\n",
    "axes[0].plot([y_measured.min(), y_measured.max()], [y_measured.min(), y_measured.max()], 'r--')\n",
    "axes[0].set_xlabel('True Values')\n",
    "axes[0].set_ylabel('Predicted Values')\n",
    "axes[0].set_title('Measured Points: True vs Predicted')\n",
    "\n",
    "# Plot 2: Prediction uncertainty\n",
    "scatter = axes[1].scatter(range(len(y_pred_all)), y_pred_all, \n",
    "                         c=y_std_all, cmap='viridis', alpha=0.6)\n",
    "axes[1].scatter(measured_indices, y_measured, c='red', s=100, marker='x', label='Measured')\n",
    "axes[1].set_xlabel('Patch Index')\n",
    "axes[1].set_ylabel('Predicted Property')\n",
    "axes[1].set_title('All Predictions with Uncertainty')\n",
    "axes[1].legend()\n",
    "plt.colorbar(scatter, ax=axes[1], label='Prediction Std')\n",
    "\n",
    "# Plot 3: Uncertainty distribution\n",
    "axes[2].hist(y_std_all, bins=30, alpha=0.7)\n",
    "axes[2].axvline(np.mean(y_std_all), color='red', linestyle='--', label=f'Mean: {np.mean(y_std_all):.3f}')\n",
    "axes[2].set_xlabel('Prediction Uncertainty')\n",
    "axes[2].set_ylabel('Count')\n",
    "axes[2].set_title('Distribution of Prediction Uncertainty')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Initial model performance metrics:\")\n",
    "print(f\"Mean prediction uncertainty: {np.mean(y_std_all):.3f}\")\n",
    "print(f\"Max prediction uncertainty: {np.max(y_std_all):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8983a633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Active Learning Loop (10 iterations)\n",
    "print(\"\\nStep 6: Active Learning Loop...\")\n",
    "\n",
    "n_active_iterations = 10\n",
    "all_measured_indices = measured_indices.copy()\n",
    "all_scalar_properties = measured_scalar_properties.copy()\n",
    "\n",
    "# Storage for tracking progress\n",
    "iteration_metrics = {\n",
    "    'iteration': [],\n",
    "    'selected_patch': [],\n",
    "    'true_property': [],\n",
    "    'predicted_property': [],\n",
    "    'prediction_uncertainty': [],\n",
    "    'acquisition_value': []\n",
    "}\n",
    "\n",
    "for iteration in range(n_active_iterations):\n",
    "    print(f\"\\n=== Active Learning Iteration {iteration + 1}/{n_active_iterations} ===\")\n",
    "    \n",
    "    # Select next measurement point\n",
    "    next_patch_idx = al_system.select_next_measurement(\n",
    "        al_system.pca_features, all_measured_indices\n",
    "    )\n",
    "    \n",
    "    if next_patch_idx is None:\n",
    "        print(\"No more candidates available!\")\n",
    "        break\n",
    "    \n",
    "    print(f\"Selected patch {next_patch_idx} for measurement\")\n",
    "    \n",
    "    # Get prediction before measurement\n",
    "    pred_mean, pred_std = al_system.predict_properties(\n",
    "        al_system.pca_features[next_patch_idx:next_patch_idx+1]\n",
    "    )\n",
    "    \n",
    "    # Acquire CBED and measure property\n",
    "    coordinates = al_system.patch_coordinates[next_patch_idx]\n",
    "    cbed_pattern = al_system.get_cbed_pattern(coordinates)\n",
    "    true_scalar_property = extract_scalar_from_cbed(cbed_pattern)\n",
    "    \n",
    "    print(f\"Predicted: {pred_mean[0]:.3f} ± {pred_std[0]:.3f}\")\n",
    "    print(f\"True value: {true_scalar_property:.3f}\")\n",
    "    print(f\"Prediction error: {abs(true_scalar_property - pred_mean[0]):.3f}\")\n",
    "    \n",
    "    # Update measurements\n",
    "    all_measured_indices.append(next_patch_idx)\n",
    "    all_scalar_properties.append(true_scalar_property)\n",
    "    \n",
    "    # Store metrics\n",
    "    iteration_metrics['iteration'].append(iteration + 1)\n",
    "    iteration_metrics['selected_patch'].append(next_patch_idx)\n",
    "    iteration_metrics['true_property'].append(true_scalar_property)\n",
    "    iteration_metrics['predicted_property'].append(pred_mean[0])\n",
    "    iteration_metrics['prediction_uncertainty'].append(pred_std[0])\n",
    "    \n",
    "    # Retrain GP model with new data\n",
    "    X_measured_updated = al_system.pca_features[all_measured_indices]\n",
    "    y_measured_updated = np.array(all_scalar_properties)\n",
    "    \n",
    "    al_system.train_gp_model(X_measured_updated, y_measured_updated)\n",
    "    \n",
    "    # Evaluate improvement\n",
    "    if iteration > 0:\n",
    "        # Compare with previous predictions\n",
    "        y_pred_new, y_std_new = al_system.predict_properties(al_system.pca_features)\n",
    "        mean_uncertainty_reduction = np.mean(y_std_all) - np.mean(y_std_new)\n",
    "        print(f\"Mean uncertainty reduction: {mean_uncertainty_reduction:.4f}\")\n",
    "        y_std_all = y_std_new  # Update for next iteration\n",
    "\n",
    "print(f\"\\nActive learning completed!\")\n",
    "print(f\"Total measurements taken: {len(all_measured_indices)}\")\n",
    "print(f\"Final property range: {np.min(all_scalar_properties):.3f} - {np.max(all_scalar_properties):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe242590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Analyze Active Learning Performance\n",
    "print(\"\\nAnalyzing Active Learning Performance...\")\n",
    "\n",
    "# Final predictions\n",
    "y_final_pred, y_final_std = al_system.predict_properties(al_system.pca_features)\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig = plt.figure(figsize=(20, 16))\n",
    "\n",
    "# 1. Learning curve - prediction errors over iterations\n",
    "ax1 = plt.subplot(3, 3, 1)\n",
    "errors = [abs(t - p) for t, p in zip(iteration_metrics['true_property'], \n",
    "                                    iteration_metrics['predicted_property'])]\n",
    "plt.plot(iteration_metrics['iteration'], errors, 'o-')\n",
    "plt.xlabel('Active Learning Iteration')\n",
    "plt.ylabel('Prediction Error')\n",
    "plt.title('Learning Curve: Prediction Errors')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Uncertainty reduction over iterations\n",
    "ax2 = plt.subplot(3, 3, 2)\n",
    "uncertainties = iteration_metrics['prediction_uncertainty']\n",
    "plt.plot(iteration_metrics['iteration'], uncertainties, 'o-', color='orange')\n",
    "plt.xlabel('Active Learning Iteration')\n",
    "plt.ylabel('Prediction Uncertainty')\n",
    "plt.title('Uncertainty Evolution')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Final predictions vs true values (for all measured points)\n",
    "ax3 = plt.subplot(3, 3, 3)\n",
    "final_measured_pred = y_final_pred[all_measured_indices]\n",
    "plt.scatter(all_scalar_properties, final_measured_pred, alpha=0.7)\n",
    "plt.plot([min(all_scalar_properties), max(all_scalar_properties)], \n",
    "         [min(all_scalar_properties), max(all_scalar_properties)], 'r--')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Final Predictions')\n",
    "plt.title('Final Model: True vs Predicted')\n",
    "\n",
    "# Calculate R²\n",
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(all_scalar_properties, final_measured_pred)\n",
    "plt.text(0.05, 0.95, f'R² = {r2:.3f}', transform=ax3.transAxes, \n",
    "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "# 4. Spatial distribution of measurements\n",
    "ax4 = plt.subplot(3, 3, 4)\n",
    "# Plot all patch locations\n",
    "all_coords = np.array(al_system.patch_coordinates)\n",
    "plt.scatter(all_coords[:, 0], all_coords[:, 1], c='lightgray', alpha=0.3, s=10, label='All patches')\n",
    "\n",
    "# Plot measured locations colored by property value\n",
    "measured_coords = np.array([al_system.patch_coordinates[i] for i in all_measured_indices])\n",
    "scatter = plt.scatter(measured_coords[:, 0], measured_coords[:, 1], \n",
    "                     c=all_scalar_properties, cmap='viridis', s=100, \n",
    "                     edgecolors='black', linewidth=1, label='Measured')\n",
    "\n",
    "plt.colorbar(scatter, ax=ax4, label='Property Value')\n",
    "plt.xlabel('X Coordinate')\n",
    "plt.ylabel('Y Coordinate')\n",
    "plt.title('Spatial Distribution of Measurements')\n",
    "plt.legend()\n",
    "\n",
    "# 5. Property histogram\n",
    "ax5 = plt.subplot(3, 3, 5)\n",
    "plt.hist(all_scalar_properties, bins=15, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Property Value')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Measured Properties')\n",
    "\n",
    "# 6. Uncertainty map\n",
    "ax6 = plt.subplot(3, 3, 6)\n",
    "# Reshape uncertainties to image-like format (approximately)\n",
    "coords_array = np.array(al_system.patch_coordinates)\n",
    "plt.scatter(coords_array[:, 0], coords_array[:, 1], c=y_final_std, \n",
    "           cmap='plasma', s=20, alpha=0.7)\n",
    "plt.colorbar(label='Prediction Uncertainty')\n",
    "plt.xlabel('X Coordinate')\n",
    "plt.ylabel('Y Coordinate')\n",
    "plt.title('Final Uncertainty Map')\n",
    "\n",
    "# 7. Property value map\n",
    "ax7 = plt.subplot(3, 3, 7)\n",
    "plt.scatter(coords_array[:, 0], coords_array[:, 1], c=y_final_pred, \n",
    "           cmap='viridis', s=20, alpha=0.7)\n",
    "plt.colorbar(label='Predicted Property')\n",
    "plt.xlabel('X Coordinate')\n",
    "plt.ylabel('Y Coordinate')\n",
    "plt.title('Final Property Prediction Map')\n",
    "\n",
    "# 8. Acquisition strategy analysis\n",
    "ax8 = plt.subplot(3, 3, 8)\n",
    "initial_indices_set = set(measured_indices[:n_initial])\n",
    "active_indices = [idx for idx in all_measured_indices if idx not in initial_indices_set]\n",
    "\n",
    "if active_indices:\n",
    "    active_coords = np.array([al_system.patch_coordinates[i] for i in active_indices])\n",
    "    plt.scatter(all_coords[:, 0], all_coords[:, 1], c='lightgray', alpha=0.3, s=10, label='All patches')\n",
    "    plt.scatter(measured_coords[:n_initial, 0], measured_coords[:n_initial, 1], \n",
    "               c='blue', s=100, marker='s', label='Initial (random)', alpha=0.7)\n",
    "    if len(active_coords) > 0:\n",
    "        plt.scatter(active_coords[:, 0], active_coords[:, 1], \n",
    "                   c='red', s=100, marker='^', label='Active learning', alpha=0.7)\n",
    "plt.xlabel('X Coordinate')\n",
    "plt.ylabel('Y Coordinate')\n",
    "plt.title('Measurement Strategy Comparison')\n",
    "plt.legend()\n",
    "\n",
    "# 9. Model improvement metrics\n",
    "ax9 = plt.subplot(3, 3, 9)\n",
    "# Plot cumulative model performance\n",
    "cumulative_r2 = []\n",
    "for i in range(n_initial, len(all_measured_indices)):\n",
    "    subset_true = all_scalar_properties[:i+1]\n",
    "    subset_pred = y_final_pred[all_measured_indices[:i+1]]\n",
    "    if len(set(subset_true)) > 1:  # Need variance for R²\n",
    "        cumulative_r2.append(r2_score(subset_true, subset_pred))\n",
    "    else:\n",
    "        cumulative_r2.append(0)\n",
    "\n",
    "if cumulative_r2:\n",
    "    plt.plot(range(n_initial, len(all_measured_indices)), cumulative_r2, 'o-')\n",
    "    plt.xlabel('Number of Measurements')\n",
    "    plt.ylabel('R² Score')\n",
    "    plt.title('Cumulative Model Performance')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ACTIVE LEARNING SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total patches available: {len(al_system.patches)}\")\n",
    "print(f\"Total measurements taken: {len(all_measured_indices)}\")\n",
    "print(f\"Measurement efficiency: {len(all_measured_indices)/len(al_system.patches)*100:.2f}%\")\n",
    "print(f\"Final model R²: {r2:.3f}\")\n",
    "print(f\"Mean prediction uncertainty: {np.mean(y_final_std):.4f}\")\n",
    "print(f\"Property value range: {np.min(all_scalar_properties):.3f} - {np.max(all_scalar_properties):.3f}\")\n",
    "print(f\"Mean absolute error: {np.mean(errors):.4f}\")\n",
    "\n",
    "# Analysis of active learning effectiveness\n",
    "initial_errors = errors[:3] if len(errors) >= 3 else errors\n",
    "final_errors = errors[-3:] if len(errors) >= 3 else errors\n",
    "print(f\"Average error (early iterations): {np.mean(initial_errors):.4f}\")\n",
    "print(f\"Average error (final iterations): {np.mean(final_errors):.4f}\")\n",
    "improvement = (np.mean(initial_errors) - np.mean(final_errors)) / np.mean(initial_errors) * 100\n",
    "print(f\"Error improvement: {improvement:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2d57a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Advanced Analysis - Structure-Property Relationships\n",
    "print(\"\\nAdvanced Analysis: Structure-Property Relationships...\")\n",
    "\n",
    "# Analyze which patch features correlate with properties\n",
    "patch_features = {}\n",
    "for i, patch in enumerate(al_system.patches):\n",
    "    patch_features[i] = {\n",
    "        'mean_intensity': np.mean(patch),\n",
    "        'std_intensity': np.std(patch),\n",
    "        'max_intensity': np.max(patch),\n",
    "        'min_intensity': np.min(patch),\n",
    "        'intensity_range': np.max(patch) - np.min(patch),\n",
    "        'texture_measure': np.mean(np.abs(np.diff(patch, axis=0))) + np.mean(np.abs(np.diff(patch, axis=1)))\n",
    "    }\n",
    "\n",
    "# Create correlation analysis for measured patches\n",
    "measured_features = {key: [patch_features[i][key] for i in all_measured_indices] \n",
    "                    for key in patch_features[0].keys()}\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "feature_names = list(measured_features.keys())\n",
    "correlations = {}\n",
    "\n",
    "for i, feature_name in enumerate(feature_names):\n",
    "    ax = axes[i]\n",
    "    feature_values = measured_features[feature_name]\n",
    "    \n",
    "    # Calculate correlation\n",
    "    correlation = np.corrcoef(feature_values, all_scalar_properties)[0, 1]\n",
    "    correlations[feature_name] = correlation\n",
    "    \n",
    "    # Scatter plot\n",
    "    ax.scatter(feature_values, all_scalar_properties, alpha=0.7)\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(feature_values, all_scalar_properties, 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax.plot(sorted(feature_values), p(sorted(feature_values)), \"r--\", alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel(feature_name.replace('_', ' ').title())\n",
    "    ax.set_ylabel('Property Value')\n",
    "    ax.set_title(f'{feature_name.replace(\"_\", \" \").title()}\\nCorrelation: {correlation:.3f}')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print correlation summary\n",
    "print(\"\\nStructure-Property Correlations:\")\n",
    "print(\"-\" * 40)\n",
    "for feature, corr in sorted(correlations.items(), key=lambda x: abs(x[1]), reverse=True):\n",
    "    print(f\"{feature.replace('_', ' ').title():20}: {corr:6.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10aebb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 12: Export Results and Create Final Report\n",
    "# import json\n",
    "# from datetime import datetime\n",
    "\n",
    "# print(\"\\nCreating Final Report and Exporting Results...\")\n",
    "\n",
    "# # Create comprehensive results dictionary\n",
    "# results = {\n",
    "#     'experiment_info': {\n",
    "#         'timestamp': datetime.now().isoformat(),\n",
    "#         'mode': 'offline' if OFFLINE_MODE else 'online',\n",
    "#         'patch_size': al_system.patch_size,\n",
    "#         'n_pca_components': al_system.n_pca_components,\n",
    "#         'total_patches': len(al_system.patches),\n",
    "#         'total_measurements': len(all_measured_indices)\n",
    "#     },\n",
    "#     'performance_metrics': {\n",
    "#         'final_r2_score': float(r2),\n",
    "#         'mean_prediction_uncertainty': float(np.mean(y_final_std)),\n",
    "#         'mean_absolute_error': float(np.mean(errors)),\n",
    "#         'property_value_range': {\n",
    "#             'min': float(np.min(all_scalar_properties)),\n",
    "#             'max': float(np.max(all_scalar_properties))\n",
    "#         }\n",
    "#     },\n",
    "#     'active_learning_progression': {\n",
    "#         'iterations': iteration_metrics['iteration'],\n",
    "#         'selected_patches': iteration_metrics['selected_patch'],\n",
    "#         'true_properties': [float(x) for x in iteration_metrics['true_property']],\n",
    "#         'predicted_properties': [float(x) for x in iteration_metrics['predicted_property']],\n",
    "#         'prediction_uncertainties': [float(x) for x in iteration_metrics['prediction_uncertainty']]\n",
    "#     },\n",
    "#     'structure_property_correlations': {k: float(v) for k, v in correlations.items()},\n",
    "#     'measured_patches': {\n",
    "#         'indices': all_measured_indices,\n",
    "#         'coordinates': [[int(coord[0]), int(coord[1])] for coord in \n",
    "#                        [al_system.patch_coordinates[i] for i in all_measured_indices]],\n",
    "#         'properties': [float(x) for x in all_scalar_properties]\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # Save results to JSON file\n",
    "# results_filename = f\"active_learning_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "# with open(results_filename, 'w') as f:\n",
    "#     json.dump(results, f, indent=2)\n",
    "\n",
    "# print(f\"Results saved to: {results_filename}\")\n",
    "\n",
    "# # Create final summary visualization\n",
    "# fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# # 1. Learning efficiency\n",
    "# ax1.plot(range(1, len(errors) + 1), errors, 'o-', linewidth=2, markersize=8)\n",
    "# ax1.set_xlabel('Active Learning Iteration')\n",
    "# ax1.set_ylabel('Prediction Error')\n",
    "# ax1.set_title('Active Learning Efficiency', fontsize=14, fontweight='bold')\n",
    "# ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# # 2. Final model performance\n",
    "# ax2.scatter(all_scalar_properties, final_measured_pred, alpha=0.7, s=100)\n",
    "# ax2.plot([min(all_scalar_properties), max(all_scalar_properties)], \n",
    "#          [min(all_scalar_properties), max(all_scalar_properties)], 'r--', linewidth=2)\n",
    "# ax2.set_xlabel('True Property Values')\n",
    "# ax2.set_ylabel('Predicted Property Values')\n",
    "# ax2.set_title(f'Final Model Performance (R² = {r2:.3f})', fontsize=14, fontweight='bold')\n",
    "\n",
    "# # 3. Most informative measurements\n",
    "# info_gain = np.array(iteration_metrics['prediction_uncertainty'])\n",
    "# sorted_indices = np.argsort(info_gain)[::-1][:5]  # Top 5 most informative\n",
    "\n",
    "# measurement_nums = np.array(iteration_metrics['iteration'])[sorted_indices]\n",
    "# uncertainties = info_gain[sorted_indices]\n",
    "# patches = np.array(iteration_metrics['selected_patch'])[sorted_indices]\n",
    "\n",
    "# bars = ax3.bar(range(len(measurement_nums)), uncertainties, \n",
    "#                color=['red', 'orange', 'gold', 'lightgreen', 'lightblue'])\n",
    "# ax3.set_xlabel('Most Informative Measurements')\n",
    "# ax3.set_ylabel('Initial Prediction Uncertainty')\n",
    "# ax3.set_title('Top 5 Most Informative Measurements', fontsize=14, fontweight='bold')\n",
    "# ax3.set_xticks(range(len(measurement_nums)))\n",
    "# ax3.set_xticklabels([f'Iter {m}\\nPatch {p}' for m, p in zip(measurement_nums, patches)])\n",
    "\n",
    "# # 4. Coverage analysis\n",
    "# ax4.hist(y_final_std, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "# ax4.axvline(np.mean(y_final_std), color='red', linestyle='--', linewidth=2, \n",
    "#            label=f'Mean: {np.mean(y_final_std):.3f}')\n",
    "# ax4.set_xlabel('Prediction Uncertainty')\n",
    "# ax4.set_ylabel('Number of Patches')\n",
    "# ax4.set_title('Final Uncertainty Distribution', fontsize=14, fontweight='bold')\n",
    "# ax4.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Print final conclusions\n",
    "# print(\"\\n\" + \"=\"*60)\n",
    "# print(\"ACTIVE LEARNING FOR STEM: STRUCTURE-PROPERTY RELATIONSHIPS\")\n",
    "# print(\"=\"*60)\n",
    "# print(\"\\nKEY FINDINGS:\")\n",
    "# print(f\"• Successfully mapped structure-property relationships using only {len(all_measured_indices)} measurements\")\n",
    "# print(f\"  out of {len(al_system.patches)} possible locations ({len(all_measured_indices)/len(al_system.patches)*100:.1f}% sampling)\")\n",
    "# print(f\"• Achieved R² = {r2:.3f} for property prediction\")\n",
    "# print(f\"• Active learning reduced prediction errors by {improvement:.1f}% compared to initial measurements\")\n",
    "# print(f\"• Strongest structure-property correlation: {max(correlations.items(), key=lambda x: abs(x[1]))[0].replace('_', ' ')} (r = {max(correlations.values(), key=abs):.3f})\")\n",
    "\n",
    "# print(f\"\\nMETHODOLOGY VALIDATION:\")\n",
    "# print(f\"• PCA successfully reduced {al_system.patch_size**2} pixel features to {al_system.n_pca_components} components\")\n",
    "# print(f\"• Explained {np.sum(al_system.pca.explained_variance_ratio_):.1%} of structural variance\")\n",
    "# print(f\"• Gaussian Process model effectively captured structure-property relationships\")\n",
    "# print(f\"• Active learning strategy efficiently explored the measurement space\")\n",
    "\n",
    "# print(f\"\\nNEXT STEPS:\")\n",
    "# print(f\"• Extend to real-time measurements with feedback control\")\n",
    "# print(f\"• Incorporate multi-property optimization\")\n",
    "# print(f\"• Apply to different material systems and length scales\")\n",
    "# print(f\"• Develop automated feature engineering for different structural motifs\")\n",
    "\n",
    "# print(f\"\\nFiles generated:\")\n",
    "# print(f\"• {results_filename} - Complete experimental results\")\n",
    "# print(f\"• Multiple analysis plots saved in notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4797fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dmclient_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
