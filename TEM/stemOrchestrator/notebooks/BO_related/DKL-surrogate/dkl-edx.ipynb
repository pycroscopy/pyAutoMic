{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc602479",
   "metadata": {},
   "source": [
    "\n",
    "# Single-objective Active learning using DigitalTwin microscope: Stochastic Variational Deep kernel learning in Gpytorch and BO loop in Botorch. [Recommended to take a GPU instance]\n",
    "Prepared by [Utkarsh Pratiush](https://github.com/utkarshp1161)\n",
    "- [parent notebook link where this is tested on Digital twin microscope](https://github.com/utkarshp1161/Active-learning-in-microscopy/blob/main/notebooks/single_objective_BO_SVDKL.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b4e6ce",
   "metadata": {},
   "source": [
    "## which cuda \n",
    "- !export CUDA_VISIBLE_DEVICES=1\n",
    "- also note that -- edx\n",
    "    - dispersion - currentlyl 20ev / channel\n",
    "    - sum - over enire counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a050e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import random\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e768599",
   "metadata": {},
   "source": [
    "## 3. Single Objective Bayesian optimization with DKL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c40a02",
   "metadata": {},
   "source": [
    "### 3a. DKL model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9623ef4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "from botorch.posteriors.gpytorch import GPyTorchPosterior\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution, VariationalStrategy\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from typing import Tuple, Optional, Dict, Union, List\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "# Simple ConvNet for feature extraction\n",
    "class ConvNetFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_channels=1, output_dim=32):\n",
    "        super(ConvNetFeatureExtractor, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.output_dim = output_dim\n",
    "        self.fc = None  # Placeholder for the fully connected layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        if (\n",
    "            len(x.shape) == 3\n",
    "        ):  # TODO: hacky way to make sure botorch acquisition function works\n",
    "            # flatten\n",
    "            batch_size, channel, mn = x.shape[0], x.shape[1], x.shape[2]\n",
    "            d = math.sqrt(mn)  ## TODO: what if mn is not a perfect square?\n",
    "            x = x.reshape(int(batch_size), int(channel), int(d), int(d))\n",
    "        # Pass through the convolutional layers\n",
    "        x = self.conv_layers(x)\n",
    "\n",
    "        # If the fully connected layer is not defined yet, initialize it dynamically******************key\n",
    "        if self.fc is None:\n",
    "            flattened_size = x.view(x.size(0), -1).size(1)\n",
    "            device = x.device  # TODO: better way to handle device\n",
    "            self.fc = nn.Linear(flattened_size, self.output_dim).to(\n",
    "                device\n",
    "            )  # Create fc layer on the correct device\n",
    "\n",
    "        # Flatten for fully connected layer\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# GP model with deep kernel using ConvNet feature extractor\n",
    "class GPModelDKL(ApproximateGP):\n",
    "    def __init__(self, inducing_points, likelihood, feature_extractor=None):\n",
    "        if feature_extractor is None:\n",
    "            feature_extractor = ConvNetFeatureExtractor(\n",
    "                input_channels=1,  # Set according to your image channels\n",
    "                output_dim=32,  # Set as per the final feature dimension\n",
    "            ).to(inducing_points.device)\n",
    "        else:\n",
    "            feature_extractor = feature_extractor.to(inducing_points.device)\n",
    "\n",
    "        # Transform inducing points with ConvNet\n",
    "        inducing_points = feature_extractor(inducing_points)\n",
    "\n",
    "        # Variational setup\n",
    "        variational_distribution = CholeskyVariationalDistribution(\n",
    "            inducing_points.size(0)\n",
    "        )\n",
    "        variational_strategy = VariationalStrategy(\n",
    "            self,\n",
    "            inducing_points,\n",
    "            variational_distribution,\n",
    "            learn_inducing_locations=True,\n",
    "        )\n",
    "\n",
    "        super(GPModelDKL, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        self.num_outputs = 1  # must be one\n",
    "        self.likelihood = likelihood\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "    def __call__(self, x, use_feature_extractor=True, *args, **kwargs):\n",
    "        ## TODO: to make it compatible with botorch acquisition function we need it to make patches internally from flattened patches\n",
    "        if use_feature_extractor:\n",
    "            if len(x.shape) == 3:\n",
    "                # flatten\n",
    "                batch_size, channel, mn = x.shape[0], x.shape[1], x.shape[2]\n",
    "                d = math.sqrt(mn)  ## TODO: what if mn is not a perfect square?\n",
    "                x = x.reshape(int(batch_size), int(channel), int(d), int(d))\n",
    "            x = self.feature_extractor(x)\n",
    "        return super().__call__(x, *args, **kwargs)\n",
    "\n",
    "    def posterior(\n",
    "        self, X, output_indices=None, observation_noise=False, *args, **kwargs\n",
    "    ) -> GPyTorchPosterior:\n",
    "        self.eval()\n",
    "        self.likelihood.eval()\n",
    "        dist = self.likelihood(self(X))\n",
    "        return GPyTorchPosterior(dist)\n",
    "\n",
    "    @property\n",
    "    def hparam_dict(self):\n",
    "        return {\n",
    "            \"likelihood.noise\": self.likelihood.noise.item(),\n",
    "            \"covar_module.base_kernel.outputscale\": self.covar_module.base_kernel.outputscale.item(),\n",
    "            \"mean_module.constant\": self.mean_module.constant.item(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef47c130",
   "metadata": {},
   "source": [
    "### 3b. Utility F:n's - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28eee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(data: np.ndarray) -> np.ndarray:  # Expected data type: torch.Tensor\n",
    "    \"\"\"Normalize data to the [0, 1] range.\"\"\"\n",
    "    return (data - data.min()) / (data.max() - data.min())\n",
    "\n",
    "\n",
    "def numpy_to_torch_for_conv(np_array) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Converts a NumPy array of shape (batch_size, a, b) to a PyTorch tensor\n",
    "    with shape (batch_size, 1, a, b) for neural network use.\n",
    "\n",
    "    Parameters:\n",
    "        np_array (np.ndarray): Input NumPy array of shape (batch_size, a, b).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Converted PyTorch tensor of shape (batch_size, 1, a, b).\n",
    "    \"\"\"\n",
    "    # Check if input is a numpy array\n",
    "    if not isinstance(np_array, np.ndarray):\n",
    "        raise TypeError(\"Input must be a NumPy array.\")\n",
    "\n",
    "    # Convert to PyTorch tensor and add a channel dimension\n",
    "    tensor = torch.from_numpy(np_array).float()  # Convert to float tensor\n",
    "    tensor = tensor.unsqueeze(1)  # Add a channel dimension at index 1\n",
    "\n",
    "    return tensor\n",
    "\n",
    "\n",
    "######################atomai utils####################################\n",
    "# Credits Maxim Ziatdinov (https://github.com/ziatdinovmax): https://github.com/pycroscopy/atomai/blob/8db3e944cd9ece68c33c8e3fcca3ef3ce9a111ea/atomai/utils/img.py#L522\n",
    "\n",
    "\n",
    "def get_coord_grid(\n",
    "    imgdata: np.ndarray, step: int, return_dict: bool = True\n",
    ") -> Union[np.ndarray, Dict[int, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Generate a square coordinate grid for every image in a stack. Returns coordinates\n",
    "    in a dictionary format (same format as generated by atomnet.predictor)\n",
    "    that can be used as an input for utility functions extracting subimages\n",
    "    and atomstat.imlocal class\n",
    "\n",
    "    Args:\n",
    "        imgdata (numpy array): 2D or 3D numpy array\n",
    "        step (int): distance between grid points\n",
    "        return_dict (bool): returns coordiantes as a dictionary (same format as atomnet.predictor)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary or numpy array with coordinates\n",
    "    \"\"\"\n",
    "    if np.ndim(imgdata) == 2:\n",
    "        imgdata = np.expand_dims(imgdata, axis=0)\n",
    "    coord = []\n",
    "    for i in range(0, imgdata.shape[1], step):\n",
    "        for j in range(0, imgdata.shape[2], step):\n",
    "            coord.append(np.array([i, j]))\n",
    "    coord = np.array(coord)\n",
    "    if return_dict:\n",
    "        coord = np.concatenate((coord, np.zeros((coord.shape[0], 1))), axis=-1)\n",
    "        coordinates_dict = {i: coord for i in range(imgdata.shape[0])}\n",
    "        return coordinates_dict\n",
    "    coordinates = [coord for _ in range(imgdata.shape[0])]\n",
    "    return np.concatenate(coordinates, axis=0)\n",
    "\n",
    "\n",
    "def get_imgstack(imgdata: np.ndarray, coord: np.ndarray, r: int) -> Tuple[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Extracts subimages centered at specified coordinates\n",
    "    for a single image\n",
    "\n",
    "    Args:\n",
    "        imgdata (3D numpy array):\n",
    "            Prediction of a neural network with dimensions\n",
    "            :math:`height \\\\times width \\\\times n channels`\n",
    "        coord (N x 2 numpy array):\n",
    "            (x, y) coordinates\n",
    "        r (int):\n",
    "            Window size\n",
    "\n",
    "    Returns:\n",
    "        2-element tuple containing\n",
    "\n",
    "        - Stack of subimages\n",
    "        - (x, y) coordinates of their centers\n",
    "    \"\"\"\n",
    "    img_cr_all = []\n",
    "    com = []\n",
    "    for c in coord:\n",
    "        cx = int(np.around(c[0]))\n",
    "        cy = int(np.around(c[1]))\n",
    "        if r % 2 != 0:\n",
    "            img_cr = np.copy(\n",
    "                imgdata[cx - r // 2 : cx + r // 2 + 1, cy - r // 2 : cy + r // 2 + 1]\n",
    "            )\n",
    "        else:\n",
    "            img_cr = np.copy(\n",
    "                imgdata[cx - r // 2 : cx + r // 2, cy - r // 2 : cy + r // 2]\n",
    "            )\n",
    "        if img_cr.shape[0:2] == (int(r), int(r)) and not np.isnan(img_cr).any():\n",
    "            img_cr_all.append(img_cr[None, ...])\n",
    "            com.append(c[None, ...])\n",
    "    if len(img_cr_all) == 0:\n",
    "        return None, None\n",
    "    img_cr_all = np.concatenate(img_cr_all, axis=0)\n",
    "    com = np.concatenate(com, axis=0)\n",
    "    return img_cr_all, com\n",
    "\n",
    "\n",
    "def extract_subimages(\n",
    "    imgdata: np.ndarray,\n",
    "    coordinates: Union[Dict[int, np.ndarray], np.ndarray],\n",
    "    window_size: int,\n",
    "    coord_class: int = 0,\n",
    ") -> Tuple[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Extracts subimages centered at certain atom class/type\n",
    "    (usually from a neural network output)\n",
    "\n",
    "    Args:\n",
    "        imgdata (numpy array):\n",
    "            4D stack of images (n, height, width, channel).\n",
    "            It is also possible to pass a single 2D image.\n",
    "        coordinates (dict or N x 2 numpy arry): Prediction from atomnet.locator\n",
    "            (can be from other source but must be in the same format)\n",
    "            Each element is a :math:`N \\\\times 3` numpy array,\n",
    "            where *N* is a number of detected atoms/defects,\n",
    "            the first 2 columns are *xy* coordinates\n",
    "            and the third columns is class (starts with 0).\n",
    "            It is also possible to pass N x 2 numpy array if the corresponding\n",
    "            imgdata is a single 2D image.\n",
    "        window_size (int):\n",
    "            Side of the square for subimage cropping\n",
    "        coord_class (int):\n",
    "            Class of atoms/defects around around which the subimages\n",
    "            will be cropped (3rd column in the atomnet.locator output)\n",
    "\n",
    "    Returns:\n",
    "        3-element tuple containing\n",
    "\n",
    "        - stack of subimages,\n",
    "        - (x, y) coordinates of their centers,\n",
    "        - frame number associated with each subimage\n",
    "    \"\"\"\n",
    "    if isinstance(coordinates, np.ndarray):\n",
    "        coordinates = np.concatenate(\n",
    "            (coordinates, np.zeros((coordinates.shape[0], 1))), axis=-1\n",
    "        )\n",
    "        coordinates = {0: coordinates}\n",
    "    if np.ndim(imgdata) == 2:\n",
    "        imgdata = imgdata[None, ..., None]\n",
    "    subimages_all, com_all, frames_all = [], [], []\n",
    "    for i, (img, coord) in enumerate(zip(imgdata, coordinates.values())):\n",
    "        coord_i = coord[np.where(coord[:, 2] == coord_class)][:, :2]\n",
    "        stack_i, com_i = get_imgstack(img, coord_i, window_size)\n",
    "        if stack_i is None:\n",
    "            continue\n",
    "        subimages_all.append(stack_i)\n",
    "        com_all.append(com_i)\n",
    "        frames_all.append(np.ones(len(com_i), int) * i)\n",
    "    if len(subimages_all) > 0:\n",
    "        subimages_all = np.concatenate(subimages_all, axis=0)\n",
    "        com_all = np.concatenate(com_all, axis=0)\n",
    "        frames_all = np.concatenate(frames_all, axis=0)\n",
    "    return subimages_all, com_all, frames_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400f79a5",
   "metadata": {},
   "source": [
    "### 3c. Utility F:n's - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dac96f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *********************************DTmic specific functions starts **********************************************#\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# setup edx acquisition\n",
    "import autoscript_tem_toolkit.vision as vision_toolkit\n",
    "from autoscript_tem_microscope_client.structures import (\n",
    "    RunOptiStemSettings,\n",
    "    RunStemAutoFocusSettings,\n",
    "    Point,\n",
    "    StagePosition,\n",
    "    AdornedImage,\n",
    "    EdsAcquisitionSettings,\n",
    "    AdornedSpectrum,\n",
    "    StemAcquisitionSettings,\n",
    "    StageVelocity,\n",
    "    EdsSpectrumImageSettings,\n",
    ")\n",
    "from autoscript_tem_microscope_client.enumerations import (\n",
    "    DetectorType,\n",
    "    CameraType,\n",
    "    OptiStemMethod,\n",
    "    OpticalMode,\n",
    "    EdsDetectorType,\n",
    "    ExposureTimeType,\n",
    ")\n",
    "\n",
    "\n",
    "def configure_acquisition(exposure_time=2):\n",
    "    \"\"\"Configure the EDS acquisition settings.\"\"\"\n",
    "    # mic_server is global variable intriduced in def run function\n",
    "    microscope = mic_server\n",
    "    eds_detector_name = microscope.detectors.eds_detectors[0]\n",
    "    eds_detector = microscope.detectors.get_eds_detector(eds_detector_name)\n",
    "    # Configure the acquisition\n",
    "    global eds_settings\n",
    "    eds_settings = EdsAcquisitionSettings()\n",
    "    eds_settings.eds_detector = eds_detector_name\n",
    "    eds_settings.dispersion = eds_detector.dispersions[-1]\n",
    "    eds_settings.shaping_time = eds_detector.shaping_times[-1]\n",
    "    eds_settings.exposure_time = exposure_time\n",
    "    eds_settings.exposure_time_type = ExposureTimeType.LIVE_TIME\n",
    "    return eds_settings\n",
    "\n",
    "\n",
    "def get_channel_index(energy_keV: float, dispersion: float, offset: float) -> int:\n",
    "    \"\"\"Convert energy (keV) into spectrum channel index.\"\"\"\n",
    "    return int(round((energy_keV - offset) / dispersion))\n",
    "\n",
    "\n",
    "import xmltodict\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_dispersion_and_offset(spectrum):\n",
    "    \"\"\"\n",
    "    Extract dispersion and offset from EDS spectrum metadata (xml).\n",
    "    Returns (dispersion_keV_per_ch, offset_keV).\n",
    "    \"\"\"\n",
    "    xml_string = spectrum.metadata.metadata_as_xml\n",
    "    metadata = xmltodict.parse(xml_string)\n",
    "    metadata = json.loads(json.dumps(metadata))\n",
    "\n",
    "    detectors = metadata[\"Metadata\"][\"Detectors\"][\"AnalyticalDetector\"]\n",
    "\n",
    "    # If only one detector, wrap it into a list\n",
    "    if isinstance(detectors, dict):\n",
    "        detectors = [detectors]\n",
    "\n",
    "    # Take the first detector (or filter by name if needed)\n",
    "    det = detectors[0]\n",
    "    dispersion = float(det.get(\"Dispersion\", 0))\n",
    "    offset = float(det.get(\"OffsetEnergy\", 0))\n",
    "\n",
    "    return dispersion, offset\n",
    "\n",
    "\n",
    "def get_eds_black_box(\n",
    "    index, indices_all, e1a, e1b, eds_settings, image_size, element=\"sum\"\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Black box function that returns a target score based on EDS peak intensities.\n",
    "    \"\"\"\n",
    "    # Move paused beam to location\n",
    "    x = int(indices_all[index, 0]) / image_size\n",
    "    y = int(indices_all[index, 1]) / image_size\n",
    "    print(\n",
    "        \"collecting spectrum at fractional coord\", x, y, \"true coord\", x * 512, y * 512\n",
    "    )\n",
    "    mic_server.optics.paused_scan_beam_position = [x, y]  # (0, 0) = top left corner\n",
    "    import time\n",
    "    # time.sleep(12)  # wait 2 seconds\n",
    "\n",
    "    # Acquire EDS spectrum\n",
    "    mic_server.optics.unblank()\n",
    "    spectrum = mic_server.analysis.eds.acquire_spectrum(eds_settings)\n",
    "    mic_server.optics.blank()\n",
    "\n",
    "    plt.imshow(img, cmap=\"gray\", origin=\"upper\")\n",
    "    plt.scatter(x * image_size, y * image_size, marker=\"o\", c=\"y\")\n",
    "\n",
    "    # Average spectrum data from 4 detectors\n",
    "    n_channels_per_detector = len(spectrum.data) // 4\n",
    "    summed_spectrum = np.zeros(n_channels_per_detector)\n",
    "\n",
    "    for i in range(4):\n",
    "        start_idx = i * n_channels_per_detector\n",
    "        end_idx = (i + 1) * n_channels_per_detector\n",
    "        summed_spectrum += spectrum.data[start_idx:end_idx]\n",
    "\n",
    "    # Use summed spectrum for analysis\n",
    "    spectrum_data = summed_spectrum\n",
    "\n",
    "    # Plot spectrum using matplotlib instead of vision_toolkit\n",
    "    dispersion, offset = get_dispersion_and_offset(spectrum)\n",
    "    energy_axis = (\n",
    "        np.arange(len(spectrum_data)) * dispersion + offset\n",
    "    ) / 1000  # 1000 for Kev\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(energy_axis, spectrum_data)\n",
    "    plt.xlabel(\"Energy (keV)\")\n",
    "    plt.ylabel(\"Counts\")\n",
    "    plt.title(\"EDS Spectrum (Summed from 4 Detectors)\")\n",
    "    plt.xlim(0, 20)  # Focus on physically relevant energy range\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # Define element peaks (in keV) with their IDs\n",
    "    element_peaks = {\n",
    "        \"Al\": {\"Al_Ka\": 1.487},\n",
    "        \"Cu\": {\"Cu_La\": 0.930, \"Cu_Ka\": 8.048, \"Cu_Kb\": 8.905},\n",
    "        \"Zr\": {\"Zr_La\": 2.04, \"Zr_Lb\": 2.12, \"Zr_Ka\": 15.77, \"Zr_Kb\": 17.67},\n",
    "        \"Sb\": {\"Sb_La\": 3.605, \"Sb_Lb\": 3.844, \"Sb_Ka\": 26.359, \"Sb_Kb\": 29.725},\n",
    "    }\n",
    "\n",
    "    # Mark element locations on the plot\n",
    "    colors = {\"Al\": \"red\", \"Cu\": \"blue\", \"Zr\": \"green\", \"Sb\": \"orange\"}\n",
    "    for elem, peaks in element_peaks.items():\n",
    "        for peak_name, energy in peaks.items():\n",
    "            if energy <= 30:  # Only mark peaks within visible range\n",
    "                plt.axvline(\n",
    "                    x=energy,\n",
    "                    color=colors[elem],\n",
    "                    linestyle=\"--\",\n",
    "                    alpha=0.7,\n",
    "                    label=f\"{elem} ({peak_name})\"\n",
    "                    if peak_name == list(peaks.keys())[0]\n",
    "                    else \"\",\n",
    "                )\n",
    "\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(\n",
    "        f\"{out_path}_time_{timestamp}_(x,y)_{x, y}disp_{dispersion}_offset_{offset}_4det_summed.png\"\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "    window_halfwidth = 3  # channels to integrate around each peak\n",
    "    total_counts = 0\n",
    "\n",
    "    # Calculate score based on element parameter\n",
    "    if element == \"sum\":\n",
    "        # Sum entire spectrum\n",
    "        total_counts = spectrum_data.sum()\n",
    "        print(f\"Total spectrum counts: {total_counts}\")\n",
    "    elif element in element_peaks:\n",
    "        # Sum counts for specified element\n",
    "        for peak_name, energy in element_peaks[element].items():\n",
    "            center_idx = get_channel_index(energy, dispersion, offset)\n",
    "            start = max(0, center_idx - window_halfwidth)\n",
    "            end = min(len(spectrum_data), center_idx + window_halfwidth)\n",
    "            peak_counts = spectrum_data[start:end].sum()\n",
    "            total_counts += peak_counts\n",
    "            print(f\"{element} {peak_name} counts: {peak_counts}\")\n",
    "        print(f\"Total {element} counts: {total_counts}\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"Warning: Element '{element}' not recognized. Available: Al, Cu, Zr, Sb, sum\"\n",
    "        )\n",
    "        total_counts = 0\n",
    "\n",
    "    # Get all element IDs for reference\n",
    "    all_element_ids = {}\n",
    "    for elem, peaks in element_peaks.items():\n",
    "        all_element_ids[elem] = list(peaks.keys())\n",
    "    print(\"Available element peak IDs:\", all_element_ids)\n",
    "\n",
    "    # Save spectrum to disk (save the summed spectrum)\n",
    "    spec_array = summed_spectrum  # Save the processed spectrum\n",
    "    dispersion, offset = get_dispersion_and_offset(spectrum=spectrum)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    np.save(\n",
    "        f\"{out_path}_time_{timestamp}_(x,y)_{x, y}disp_{dispersion}_offset_{offset}_4det_summed.npy\",\n",
    "        spec_array,\n",
    "    )\n",
    "    print(\"total counts\", total_counts)\n",
    "    return total_counts\n",
    "\n",
    "\n",
    "# def get_eds_black_box(index, indices_all, e1a, e1b, eds_settings, image_size) -> float:\n",
    "#     \"\"\"\n",
    "#     Black box function that returns a target score simulates the blackbox function\n",
    "#     \"\"\"\n",
    "\n",
    "#     e_start,e_end = e1a, e1b\n",
    "#     # move paused beam to the location index\n",
    "#     x=int(indices_all[index, 0])/image_size######### TODO: check if x anf y needs to be flipped\n",
    "#     y=int(indices_all[index, 1])/image_size\n",
    "#     mic_server.optics.paused_scan_beam_position = list(x, y)#----> (0, 0) should be top left corner\n",
    "\n",
    "#     # collect eds\n",
    "#     microscope.optics.unblank()\n",
    "#     spectrum = mic_server.analysis.eds.acquire_spectrum(eds_settings)\n",
    "#     microscope.optics.blank()\n",
    "#     score = spectrum[e_start:e_end].sum()\n",
    "\n",
    "#     return score\n",
    "\n",
    "\n",
    "## a) evaluations metrics like nlpd, mse ----\n",
    "def calculate_mse(y_true, y_pred):\n",
    "    \"\"\"Calculate Mean Squared Error (MSE)\"\"\"\n",
    "    # Smaller values indicate better predictions.\n",
    "    # Squaring ensures that positive and negative errors don't cancel out.\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    return mse\n",
    "\n",
    "\n",
    "def calculate_nlpd(y_true, y_pred_mean, y_pred_var):\n",
    "    \"\"\"Calculate Negative Log Predictive Density (NLPD)\"\"\"\n",
    "    # NLPD evaluates how well the predicted probability distribution matches the true values.\n",
    "    # Lower NLPD indicates a better match, accounting for both the mean and uncertainty.\n",
    "    nlpd = 0.5 * torch.log(2 * torch.pi * y_pred_var) + 0.5 * (\n",
    "        (y_true - y_pred_mean) ** 2 / y_pred_var\n",
    "    )\n",
    "    return nlpd.mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9472b96",
   "metadata": {},
   "source": [
    "### 3d. Utility F:n's - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8feccfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def calculate_scores_for_patches(\n",
    "    unacquired_indices,\n",
    "    indices_all,\n",
    "    e1a,\n",
    "    e1b,\n",
    "    black_box_fn=get_eds_black_box,\n",
    "    debug=True,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculate the score for each patch using the black_box function.\n",
    "\n",
    "    Parameters:\n",
    "    - patches: Tensor of all data patches.\n",
    "\n",
    "    Returns:\n",
    "    - scores: List of scores for each patch.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for i in unacquired_indices:\n",
    "        score = black_box_fn(i, indices_all, e1a, e1b)  # Calculate score for each patch\n",
    "        scores.append(score)\n",
    "    return torch.tensor(scores)  # Return as a tensor for compatibility\n",
    "\n",
    "\n",
    "def update_acquired(\n",
    "    acquired_data,\n",
    "    unacquired_indices,\n",
    "    selected_indices,\n",
    "    indices_all,\n",
    "    e1a,\n",
    "    e1b,\n",
    "    eds_settings,\n",
    "    image_size,\n",
    "    black_box_fn=get_eds_black_box,\n",
    ") -> (np.array, list):\n",
    "    for idx in (\n",
    "        selected_indices\n",
    "    ):  # TODO: It queries the black box everytime on already acquired points:\n",
    "        acquired_data[idx] = black_box_fn(\n",
    "            idx, indices_all, e1a, e1b, eds_settings, image_size\n",
    "        )\n",
    "    unacquired_indices = [\n",
    "        idx for idx in unacquired_indices if idx not in selected_indices\n",
    "    ]\n",
    "\n",
    "    return acquired_data, unacquired_indices\n",
    "\n",
    "\n",
    "def load_image_and_features(\n",
    "    img: np.ndarray, window_size: int\n",
    ") -> (np.ndarray, np.ndarray):\n",
    "    coordinates = get_coord_grid(img, step=1, return_dict=False)\n",
    "    features_all, coords, _ = extract_subimages(img, coordinates, window_size)\n",
    "    features_all = features_all[:, :, :, 0]\n",
    "    coords = np.array(coords, dtype=int)\n",
    "    norm_ = lambda x: (x - x.min()) / np.ptp(x)\n",
    "    features = norm_(features_all)\n",
    "    return features, coords  # shapes (3366, 5, 5) and (3366, 2)\n",
    "\n",
    "\n",
    "def prepare_data_from_microscope(\n",
    "    window_size: int, haadf: np.ndarray\n",
    ") -> (np.ndarray, np.ndarray):\n",
    "    global img  # TODO: better way to deal with this --> at this point need to plot it when collcting spectrum\n",
    "    img = haadf\n",
    "    features, indices_all = load_image_and_features(img, window_size)\n",
    "    return img, features, indices_all  # shapes (55, 70), (3366, 5, 5) and (3366, 2)\n",
    "\n",
    "\n",
    "def embeddings_and_predictions(\n",
    "    model, patches, device=\"cpu\"\n",
    ") -> (torch.Tensor, torch.Tensor):\n",
    "    \"\"\"\n",
    "    Get predictions from the trained model\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    patches = patches.to(device)\n",
    "    with torch.no_grad():\n",
    "        predictions = model(patches)\n",
    "        embeddings = (\n",
    "            model.feature_extractor(patches).view(patches.size(0), -1).cpu().numpy()\n",
    "        )\n",
    "    return predictions, embeddings\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    acquired_data,\n",
    "    patches,\n",
    "    feature_extractor,\n",
    "    device=\"cpu\",\n",
    "    num_epochs=50,\n",
    "    log_interval=5,\n",
    "    scalarizer_zero=False,\n",
    ") -> ApproximateGP:\n",
    "    X_train = torch.stack([patches[idx] for idx in acquired_data]).to(device)\n",
    "    y_train = torch.tensor(list(acquired_data.values()), dtype=torch.float32).to(device)\n",
    "    if scalarizer_zero:\n",
    "        y_train = torch.zeros_like(y_train)\n",
    "\n",
    "    else:\n",
    "        # Normalize y_train\n",
    "\n",
    "        y_train = (y_train - y_train.min()) / (y_train.max() - y_train.min())\n",
    "\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood().to(device)\n",
    "    inducing_points = X_train[:10]\n",
    "    model = GPModelDKL(\n",
    "        inducing_points=inducing_points,\n",
    "        likelihood=likelihood,\n",
    "        feature_extractor=feature_extractor,\n",
    "    ).to(device)\n",
    "\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    for epoch in tqdm(range(1, num_epochs + 1), desc=\"Training Progress\"):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_train)\n",
    "\n",
    "        loss = -mll(output, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0388d5",
   "metadata": {},
   "source": [
    "### 3e. Bayesian optimization loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d6ca27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stemOrchestrator.acquisition import TFacquisition, DMacquisition\n",
    "from stemOrchestrator.simulation import DMtwin\n",
    "from autoscript_tem_microscope_client.enumerations import EdsDetectorType\n",
    "from stemOrchestrator.process import HAADF_tiff_to_png, tiff_to_png\n",
    "from autoscript_tem_microscope_client import TemMicroscopeClient\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "plot = plt\n",
    "from typing import Dict\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0ed20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(config) -> None:\n",
    "    # Extract all configuration variables\n",
    "    ip = config[\"ip\"]\n",
    "    port = config[\"port\"]\n",
    "    seed = config[\"seed\"]\n",
    "    seed_pts = config[\"seed_pts\"]\n",
    "    budget = config[\"budget\"]\n",
    "    out_dir_parent = config[\"out_dir_parent\"]\n",
    "    dataset_name = config[\"dataset_name\"]\n",
    "    device = config[\"device\"]\n",
    "    num_epochs = config[\"num_epochs\"]\n",
    "    normalize_data_flag = config[\"normalize_data\"]\n",
    "    window_size = config[\"window_size\"]\n",
    "    scal_stem = config[\"scal_stem\"]\n",
    "    haadf_exposure = config[\"haadf_exposure\"]\n",
    "    haadf_resolution = config[\"haadf_resolution\"]\n",
    "    edx_exposure = config[\"edx_exposure\"]\n",
    "\n",
    "    scalarizer_zero = False  # TODO: deafult value to zero -- so passed to train_model function --> better way to handel\n",
    "\n",
    "    if scal_stem is not None:  ## only for pfm: TODO : find better to accomodiate this\n",
    "        if scal_stem == \"sum\":\n",
    "            black_box_fn = get_eds_black_box\n",
    "            # energy_range to sum in--> TODO: idea to sum for an element like zirconium\n",
    "            e1a = 0\n",
    "            e1b = 20  ###### till dispersion?\n",
    "\n",
    "    else:\n",
    "        print(\"what scalarizer you want\")\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    res_dir = (\n",
    "        Path(out_dir_parent)\n",
    "        / f\"Dataset_seed{seed}_{dataset_name}_BO_{seed_pts}_epochs{num_epochs}_budget_{budget}_{scal_stem}_ws{window_size}_{timestamp}\"\n",
    "    )\n",
    "    res_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Connect to the microscope server\n",
    "    global mic_server  # TODO: later see better way to do this\n",
    "    microscope = TemMicroscopeClient()\n",
    "    microscope.connect(ip, port=port)  # 7521 on velox  computer\n",
    "    mic_server = microscope\n",
    "    tf_acquisition = TFacquisition(microscope=microscope)\n",
    "\n",
    "    # get haadf for dkl\n",
    "    haadf_np_array, haadf_tiff_name = tf_acquisition.acquire_haadf(\n",
    "        exposure=haadf_exposure, resolution=haadf_resolution, folder_path=out_dir_parent\n",
    "    )\n",
    "    image_size = haadf_resolution\n",
    "    HAADF_tiff_to_png(out_dir_parent + haadf_tiff_name)\n",
    "\n",
    "    # Prepare features and indices from microscope image\n",
    "    img, features, indices_all = prepare_data_from_microscope(\n",
    "        window_size=window_size, haadf=haadf_np_array\n",
    "    )\n",
    "\n",
    "    ############################################\n",
    "    patches = numpy_to_torch_for_conv(features)\n",
    "    patches = patches.to(device)\n",
    "\n",
    "    if normalize_data_flag:\n",
    "        patches = normalize_data(patches)\n",
    "\n",
    "    feature_extractor = ConvNetFeatureExtractor(input_channels=1, output_dim=2).to(\n",
    "        device\n",
    "    )\n",
    "    acquired_data = {}\n",
    "    unacquired_indices = list(\n",
    "        range(len(indices_all))\n",
    "    )  ####### TODO: need to change later to use the indices_all\n",
    "\n",
    "    # points to randompy sample from\n",
    "    selected_indices = random.sample(unacquired_indices, seed_pts)\n",
    "    seed_indices = selected_indices\n",
    "\n",
    "    ######### queries microscope to get measuremnt on seed points\n",
    "    eds_settings = configure_acquisition(exposure_time=edx_exposure)\n",
    "    acquired_data, unacquired_indices = update_acquired(\n",
    "        acquired_data,\n",
    "        unacquired_indices,\n",
    "        selected_indices,\n",
    "        indices_all,\n",
    "        e1a,\n",
    "        e1b,\n",
    "        eds_settings,\n",
    "        image_size,\n",
    "        black_box_fn=black_box_fn,\n",
    "    )\n",
    "\n",
    "    from botorch.acquisition import LogExpectedImprovement  # ExpectedImprovement\n",
    "\n",
    "    mean_y_pred_mean_al = []\n",
    "    mean_y_pred_variance_al = []\n",
    "    # mae_list = []\n",
    "    # nlpd_list = []\n",
    "    # Start Bayesian Optimization loop\n",
    "    for step in range(budget):\n",
    "        # Train the DKL model\n",
    "        model = train_model(\n",
    "            acquired_data,\n",
    "            patches,\n",
    "            feature_extractor,\n",
    "            device=device,\n",
    "            num_epochs=num_epochs,\n",
    "            scalarizer_zero=scalarizer_zero,\n",
    "        )\n",
    "        model.eval()\n",
    "\n",
    "        # Wrap the model and likelihood in the BoTorch model ------> Ithink not needed as have approxiamateGP--> check later\n",
    "\n",
    "        # Prepare candidate set (unacquired patches)\n",
    "        candidate_indices = unacquired_indices\n",
    "\n",
    "        X_candidates = torch.stack([patches[idx] for idx in candidate_indices]).to(\n",
    "            device\n",
    "        )\n",
    "        X_candidates = X_candidates.reshape(\n",
    "            -1, 1, window_size * window_size\n",
    "        )  # Note this is when using acq f:n directly and not invoking  optimize_acqf_discrete\n",
    "\n",
    "        y_train = torch.tensor(list(acquired_data.values()), dtype=torch.float32).to(\n",
    "            device\n",
    "        )\n",
    "        y_train = (y_train - y_train.min()) / (y_train.max() - y_train.min())\n",
    "\n",
    "        acq_func = LogExpectedImprovement(model=model, best_f=y_train.max().to(device))\n",
    "\n",
    "        acq_values_candidates = acq_func(X_candidates)  # Only for candidates\n",
    "        best_idx = torch.argmax(acq_values_candidates).item()\n",
    "        selected_candidate = X_candidates[best_idx]\n",
    "        selected_index = candidate_indices[best_idx]\n",
    "        # Map selected tensors back to indices\n",
    "        selected_indices = [\n",
    "            selected_index\n",
    "        ]  #### can be multiple indices if batch acquisition\n",
    "\n",
    "        # Update acquired data with new observations\n",
    "        acquired_data, unacquired_indices = update_acquired(\n",
    "            acquired_data,\n",
    "            unacquired_indices,\n",
    "            selected_indices,\n",
    "            indices_all,\n",
    "            e1a,\n",
    "            e1b,\n",
    "            eds_settings,\n",
    "            image_size,\n",
    "            black_box_fn=black_box_fn,\n",
    "        )\n",
    "\n",
    "        print(f\"**************************done BO step {step + 1}\")\n",
    "\n",
    "        predictions, embeddings = embeddings_and_predictions(\n",
    "            model, patches, device=device\n",
    "        )\n",
    "\n",
    "        y_pred_mean = predictions.mean\n",
    "        y_pred_var = predictions.variance\n",
    "\n",
    "        candidate_acq_dict = {\n",
    "            candidate_indices[i]: acq_values_candidates[i].item()\n",
    "            for i in range(len(candidate_indices))\n",
    "        }\n",
    "\n",
    "        # Calculate MSE and NLPD\n",
    "        # mse = calculate_mse(true_scalarizer.cpu(), y_pred_mean.cpu())\n",
    "        # mae = np.sqrt(mse)\n",
    "        # nlpd = calculate_nlpd(true_scalarizer.cpu(), y_pred_mean.cpu(), y_pred_var.cpu())\n",
    "\n",
    "        # Fill the prediction image with predicted mean values\n",
    "        acq_fn_img = np.zeros((img.shape[0], img.shape[1]))\n",
    "        y_pred_mean_img = np.zeros((img.shape[0], img.shape[1]))\n",
    "        y_pred_var_img = np.zeros((img.shape[0], img.shape[1]))\n",
    "        # Fill the prediction image with predicted mean values\n",
    "        for j in range(len(indices_all)):\n",
    "            # Fill acq_fn_img: non-zero only for candidates, zero for acquired points\n",
    "            if j in candidate_acq_dict:\n",
    "                acq_fn_img[indices_all[j][0], indices_all[j][1]] = candidate_acq_dict[j]\n",
    "            # else remains 0 (for acquired points)\n",
    "\n",
    "            y_pred_mean_img[indices_all[j][0], indices_all[j][1]] = y_pred_mean[j]\n",
    "            y_pred_var_img[indices_all[j][0], indices_all[j][1]] = y_pred_var[j]\n",
    "\n",
    "        # Display the images\n",
    "        fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "        # original overview image\n",
    "        im0 = axs[0, 0].imshow(img, cmap=\"gray\", origin=\"upper\")\n",
    "        axs[0, 0].set_title(\"Original Image with next point selection\")\n",
    "        axs[0, 0].scatter(\n",
    "            [int(indices_all[selected_indices[0]][0])],\n",
    "            [int(indices_all[selected_indices[0]][1])],\n",
    "            color=\"yellow\",\n",
    "            marker=\"x\",\n",
    "        )\n",
    "        fig.colorbar(im0, ax=axs[0, 0])\n",
    "\n",
    "        # preicted mean\n",
    "        im1 = axs[0, 1].imshow(y_pred_mean_img, cmap=\"viridis\", origin=\"upper\")\n",
    "        axs[0, 1].set_title(\"Predicted Mean\")\n",
    "        fig.colorbar(im1, ax=axs[0, 1])\n",
    "\n",
    "        # predicted variance\n",
    "        im2 = axs[1, 0].imshow(y_pred_var_img, cmap=\"viridis\", origin=\"upper\")\n",
    "        axs[1, 0].set_title(\"Predicted Variance\")\n",
    "        fig.colorbar(im2, ax=axs[1, 0])\n",
    "\n",
    "        # Acquisition Function\n",
    "        im3 = axs[1, 1].imshow(acq_fn_img, cmap=\"viridis\", origin=\"upper\")\n",
    "        axs[1, 1].set_title(\"Acquisition Function\")\n",
    "        axs[1, 1].scatter(\n",
    "            [int(indices_all[selected_indices[0]][0])],\n",
    "            [int(indices_all[selected_indices[0]][1])],\n",
    "            color=\"red\",\n",
    "            marker=\"x\",\n",
    "            s=100,\n",
    "            label=\"Selected\",\n",
    "        )\n",
    "\n",
    "        fig.colorbar(im3, ax=axs[1, 1])\n",
    "\n",
    "        for ax in axs.flat:\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "        # fig.suptitle(f'MAE: {mae:.4f}, NLPD: {nlpd:.4f}', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(Path(res_dir) / f\"_BO_step{step}.png\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        # Save predictions as a .pkl file\n",
    "        predictions_data = {\n",
    "            \"acq_fn_img\": acq_fn_img,\n",
    "            \"y_pred_mean_img\": y_pred_mean_img,\n",
    "            \"y_pred_var_img\": y_pred_var_img,\n",
    "            \"embeddings\": embeddings,\n",
    "        }\n",
    "\n",
    "        with open(Path(res_dir) / f\"predictions_BO_step{step}.pkl\", \"wb\") as f:\n",
    "            pickle.dump(predictions_data, f)\n",
    "\n",
    "        mean_y_pred_mean_al.append(y_pred_mean.mean().cpu())\n",
    "        mean_y_pred_variance_al.append(y_pred_var.mean().cpu())\n",
    "\n",
    "        # imshow 4 images: img, pred_mean_img, pred_var_img, true_scal_img\n",
    "\n",
    "    # Save predictions as a .pkl file\n",
    "    Active_learning_statistics = {\n",
    "        \"img\": img,\n",
    "        \"features\": features,\n",
    "        \"indices_all\": np.array(indices_all),\n",
    "        \"seed_indices\": np.array(seed_indices),\n",
    "        \"unacquired_indices\": np.array(unacquired_indices),\n",
    "        \"mean_y_pred_mean_al\": np.array(mean_y_pred_mean_al),\n",
    "        \"mean_y_pred_variance_al\": np.array(mean_y_pred_variance_al),\n",
    "        # \"mae\": np.array(mae_list),\n",
    "        # \"nlpd\": np.array(nlpd_list)\n",
    "    }\n",
    "\n",
    "    with open(Path(res_dir) / f\"Active_learning_statistics.pkl\", \"wb\") as f:\n",
    "        pickle.dump(Active_learning_statistics, f)\n",
    "\n",
    "    ##############################\n",
    "\n",
    "    predictions_data = Active_learning_statistics\n",
    "    # Extract necessary data\n",
    "    img = np.array(\n",
    "        predictions_data[\"img\"]\n",
    "    )  # Image or grid for background visualization\n",
    "    seed_indices = np.array(\n",
    "        predictions_data[\"seed_indices\"]\n",
    "    )  # Initial sampled indices (referring to positions in indices_all)\n",
    "    unacquired_indices = np.array(\n",
    "        predictions_data[\"unacquired_indices\"]\n",
    "    )  # Remaining indices\n",
    "    indices_all = np.array(\n",
    "        predictions_data[\"indices_all\"]\n",
    "    )  # All possible indices (coordinates)\n",
    "\n",
    "    # Map seed_indices and unacquired_indices to their coordinates in indices_all\n",
    "    seed_coords = indices_all[seed_indices]\n",
    "    unacquired_coords = indices_all[unacquired_indices]\n",
    "\n",
    "    # Calculate acquired indices as the complement of unacquired and seed indices\n",
    "    acquired_indices = np.setdiff1d(\n",
    "        np.arange(indices_all.shape[0]),\n",
    "        np.union1d(seed_indices, unacquired_indices),\n",
    "        assume_unique=True,\n",
    "    )\n",
    "    acquired_coords = indices_all[acquired_indices]\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # Display the image or grid as the background\n",
    "    plt.imshow(img, cmap=\"gray\", origin=\"upper\")\n",
    "\n",
    "    # Plot the seed points in blue\n",
    "    plt.scatter(\n",
    "        seed_coords[:, 0], seed_coords[:, 1], c=\"b\", label=\"Seed Points\", marker=\"o\"\n",
    "    )\n",
    "\n",
    "    time_order = np.arange(len(acquired_coords))  # Create a sequence representing time\n",
    "    scatter = plt.scatter(\n",
    "        acquired_coords[:, 0],\n",
    "        acquired_coords[:, 1],\n",
    "        c=time_order,\n",
    "        cmap=\"bwr\",\n",
    "        label=\"Acquired Points\",\n",
    "        marker=\"x\",\n",
    "    )\n",
    "\n",
    "    # Plot the unacquired points in green\n",
    "    # plt.scatter(unacquired_coords[:, 1], unacquired_coords[:, 0], c=\"g\", label=\"Unacquired Points\", marker=\"+\")\n",
    "\n",
    "    # Set plot labels and legend\n",
    "    plt.xlabel(\"X-axis\")\n",
    "    plt.ylabel(\"Y-axis\")\n",
    "    plt.title(\"Active Learning Trajectory\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    # Add a colorbar and label it as \"Steps\"\n",
    "    cbar = plt.colorbar(scatter)\n",
    "    cbar.set_label(\"Steps\")\n",
    "\n",
    "    plt.savefig(Path(res_dir) / \"AL_traj.png\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Extract data for learning curve\n",
    "    mean_y_pred_mean_al = np.array(predictions_data[\"mean_y_pred_mean_al\"])\n",
    "    mean_y_pred_variance_al = np.array(predictions_data[\"mean_y_pred_variance_al\"])\n",
    "    # mae_list = np.array(predictions_data[\"mae\"])\n",
    "    # nlpd_list = np.array(predictions_data[\"nlpd\"])\n",
    "\n",
    "    steps = np.arange(\n",
    "        len(mean_y_pred_mean_al)\n",
    "    )  # Assuming the steps are sequential indices\n",
    "\n",
    "    # Calculate the upper and lower bounds using variance\n",
    "    upper_bound = mean_y_pred_mean_al + np.sqrt(mean_y_pred_variance_al)\n",
    "    lower_bound = mean_y_pred_mean_al - np.sqrt(mean_y_pred_variance_al)\n",
    "\n",
    "    # Plot the learning curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot the mean predictions\n",
    "    plt.plot(\n",
    "        steps, mean_y_pred_mean_al, label=\"Mean Prediction\", color=\"blue\", linewidth=2\n",
    "    )\n",
    "\n",
    "    # Fill between the upper and lower bounds to represent variance\n",
    "    plt.fill_between(\n",
    "        steps,\n",
    "        lower_bound,\n",
    "        upper_bound,\n",
    "        color=\"blue\",\n",
    "        alpha=0.2,\n",
    "        label=\"Variance (Â±1 std)\",\n",
    "    )\n",
    "\n",
    "    # Add labels, title, and legend\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Mean Prediction\")\n",
    "    plt.title(\"Learning Curve with Variance\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(Path(res_dir) / \"AL_learning_curve.png\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # plt.plot(steps, mae_list, color=\"red\", linewidth=2)\n",
    "\n",
    "    # plt.xlabel(\"Steps\")\n",
    "    # plt.ylabel(\"Mean absolute ERROR\")\n",
    "    # plt.grid(True)\n",
    "    # plt.savefig(Path(res_dir) / \"AL_error_curve.png\")\n",
    "    # plt.show()\n",
    "    # plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aded14f6",
   "metadata": {},
   "source": [
    "### 3f. Set parameters and Run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5348e89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stemOrchestrator.logging_config import setup_logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "exp_name = \"edx-haadf\"\n",
    "data_folder = f\"./{exp_name}_{current_time}/\"\n",
    "os.makedirs(name=data_folder, exist_ok=True)\n",
    "out_path = data_folder\n",
    "setup_logging(out_path=out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9896c9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stemOrchestrator.acquisition import TFacquisition, DMacquisition\n",
    "from stemOrchestrator.simulation import DMtwin\n",
    "from stemOrchestrator.process import HAADF_tiff_to_png, tiff_to_png\n",
    "from autoscript_tem_microscope_client import TemMicroscopeClient\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "plot = plt\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da51a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "ip = os.getenv(\"MICROSCOPE_IP\")\n",
    "port = os.getenv(\"MICROSCOPE_PORT\")\n",
    "\n",
    "if not ip or not port:\n",
    "    secret_path = Path(\"../../../config_secret.json\")\n",
    "    if secret_path.exists():\n",
    "        with open(secret_path, \"r\") as f:\n",
    "            secret = json.load(f)\n",
    "            ip = ip or secret.get(\"ip_TF\")\n",
    "            port = port or secret.get(\"port_TF\")\n",
    "print(ip, port)\n",
    "\n",
    "config = {\n",
    "    \"ip\": ip,\n",
    "    \"port\": port,\n",
    "    \"haadf_exposure\": 40e-8,  # micro-seconds per pixel\n",
    "    \"haadf_resolution\": 512,  # square\n",
    "    \"edx_exposure\": 3e-3,  # seconds\n",
    "    \"seed\": 5,\n",
    "    \"seed_pts\": 5,  # How many points you want to start your BO with?\n",
    "    \"budget\": 5,  # How many experimental budget you have?\n",
    "    \"out_dir_parent\": out_path,  # recommended : leave as is\n",
    "    \"dataset_name\": \"live_mic\",  # name of data to be loaded in DTmicroscope\n",
    "    \"device\": \"cuda\",\n",
    "    \"num_epochs\": 1,  # Number of epoch the dkl model trains at each experimental step - Might need tuning based on data\n",
    "    \"normalize_data\": True,\n",
    "    \"window_size\": 16,  # For square patches - structure property relationship\n",
    "    \"scal_stem\": \"sum\",  # What physics interested in? options on this data: \"loop_area\", \"loop_height\", \"positive_nucleation_bias\", \"negative_nucleation_bias\"\n",
    "}\n",
    "run(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0695f87f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476309b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stemorchestrator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
